[メインページ](../../index.markdown)

[章目次](./chap10.md)
## 10.4. 関係抽出

グラフニューラルネットワークは，関係抽出(RE;Relation Extraction)タスクにも適用される(Zhang *et al*., 2018c; Fu *et al*., 2019; Guo *et al*., 2019; Zhu *et al*., 2019b; Sahu *et al*., 2019; Sun *et al*., 2019a; Zhang *et al*., 2019d)． これらの研究の中で，Zhang *et al*.(2018c)やFu *et al*.(2019)，Guo *et al*.(2019)では，関係抽出タスクに構文情報を取り入れるためのグラフニューラルネットワークモデル（すなわち，Marcheggiani and Titov.(2017)の式(10.1)やそれを改良したもの）を採用している． GNNをREタスクに適用した最初の研究はZhang *et al*.(2018c)で紹介されている． 本節では，REタスクについて簡単に説明した後，Zhang *et al*.(2018c)での研究を例に挙げ，GNNがどのようにREに適用されるかを見ていくことにしよう．

関係抽出(RE)タスクでは，文章内の2つのエンティティ（主語や目的語）の間に関係が存在するかどうかを判別する． より正確には以下のように定義される：まず， $\symscr{W}=[w_1,\dots,w_n]$ は文章全体を表すものとし， $w_i$ は文中の $i$ 番目のトークン（単語）とする．エンティティは，文章内で連続した単語から成るスパン(span)である
[^4]
．具体的には，主語のエンティティ(連続する単語系列)はスパン $\symscr{W}\_s=[w_{s1}\,:\,w_{s2}]$ として表され，同様に，目的語のエンティティはスパン $\symscr{W}\_o=[w_{o1}\,:\,w_{o2}]$ として表現される
[^5]
．これらの定義の下で，関係抽出タスクの目的は，文章 $\symscr{W}$ が与えられたときに主語のエンティティ $\symscr{W}\_s$ と目的語のエンティティ $\symscr{W}\_o$ の関係を予測することである（なお， $\symscr{W}\_s$ と $\symscr{W}\_o$ は既に与えられているものとする）． 抽出対象の関係は事前に定義された集合 $\symscr{R}$ から来ており，この集合には，2つのエンティティの間に関係がないことを示す「無関係(no relation)」という特殊な関係も含まれている． 関係抽出の問題は，Zhang *et al*.(2018c)では分類問題として扱われている． このときの入力は，文章 $\symscr{W}$ の表現，主語エンティティ $\symscr{W}\_s$ の表現，目的語エンティティ $\symscr{W}\_o$ の表現をすべて連結したものである． 出力ラベルは関係の集合 $\symscr{R}$ の要素である． つまり，エンティティのペアに成立する関係の予測は，以下のようにパラメータ $\symscr{\Theta}\_{\text{FFNN}}$ を持つ順伝播型ニューラルネットワーク(FFNN)を通じて行われる：  

$$
 \symbf{p} = \operatorname{softmax}([\symbf{F}_{\text{sent}}, \symbf{F}_{\text{s}}, \symbf{F}_{\text{o}}]\symbf{\Theta}_{\text{FFNN}}) $$


  ここで， $\operatorname{softmax}$ はソフトマックス関数， $\symbf{p}$ は関係の集合 $\symscr{R}$ に対する確率分布を示しており， $\symbf{F}\_{\text{sent}}$ ， $\symbf{F}\_{\text{s}}$ ， $\symbf{F}\_{\text{o}}$ はそれぞれ文章全体の表現，主語エンティティの表現，および目的語エンティティの表現を表している． 文脈情報だけでなく，文章の構文構造も捉えるために，Marcheggiani and Titov.(2017)（つまり，10.2節で紹介したSRLタスク用のモデル）でのモデルと非常に類似した手続きが用いて単語表現を学習し，それを用いて文章全体の表現，主語エンティティの表現，目的語エンティティの表現が学習される． SRLタスクのモデルとの主な違いは，式(10.1)を使って表現を更新する際に，自身のノードを含む自己ループが導入されるという点である． 言い換えれば，RE用のモデルにおける式(10.1)の $\symscr{N}(v_i)$ は，ノード $v_i$ およびそのノードの入・出近傍となる( $\symscr{N}(v_i)\cup\left\\{v_i\right\\}$ )． また，Zhang *et al*.(2018c)では，エッジの方向やラベル情報を含めてもREタスクの性能向上の助けにならないことを実証的に示している．

上述の $L$ 個のグラフフィルタリング層から成るモデルから単語表現を得た後，以下の最大プーリングを用いることによって，文章の表現，主語エンティティの表現，目的語エンティティの表現を得る：

 $$
 
\tag{10.2}
    
\begin{aligned}
        \symbf{F}_{\text{sent}} &= \max(\symbf{F}^{(L)})\\
        \symbf{F}_{\text{s}} &= \max(\symbf{F}^{(L)}[s1\,:\,s2])\\
        \symbf{F}_{\text{o}} &= \max(\symbf{F}^{(L)}[o1\,:\,o2])
    
\end{aligned}
$$

 

ここで， $\symbf{F}^{(L)}$ ， $\symbf{F}^{(L)}[s1\,:\,s2]$ ，および $\symbf{F}^{(L)}[o1\,:\,o2]$ は，それぞれ文章全体，主語エンティティ，目的語エンティティに対する単語表現の系列を表している． 最大プーリング操作は各次元の中での最大値を取り出すため，その結果として各単語表現と同じ次元を持つベクトルが得られる


[メインページ](../../index.markdown)

[章目次](./chap10.md)

[前の節へ](./subsection_03.md) [次の節へ](./subsection_05.md)

[^4]: 訳注：例えば，"The quick brown fox jumps over the lazy dog"という文章では，"The quick brown fox"や"the lazy dog"はスパンの一例である．また，個々の単語自体もスパンを形成することができる．注目すべき点は，スパンは文章の特定の範囲を指すだけで，それが具体的にどのような意味を持つか（特定の種類のエンティティを示しているかどうかなど）については言及していないということである．
[^5]: 訳注：例えば，  $\symscr{W} = [\text{The}, \text{quick}, \text{brown}, \text{fox}, \text{jumps}, \text{over}, \text{the}, \text{lazy}, \text{dog}]$  のとき，主語のエンティティと目的語のエンティティは以下のようになる：  $\symscr{W}\_{\text{s}}=[\text{The}, \text{quick}, \text{brown}, \text{fox}],\qquad\symscr{W}\_{\text{o}}=[\text{the}, \text{lazy}, \text{dog}]$ 

[メインページ](../../index.markdown)

[章目次](./chap10.md)
## 10.3. ニューラル機械翻訳

自然言語処理における重要なタスクの一つに機械翻訳がある． 深層学習の発展に伴い，ニューラルネットワークは機械翻訳のために広く採用されてきた． これらのニューラルネットワークベースに基づくモデルは，ニューラル機械翻訳モデルとよばれ，通常，エンコーダー・デコーダー形式をとる． エンコーダーは，翻訳元の言語の単語列を入力として受け取り，その系列内の各単語の表現を出力する． その後，エンコーダーからの表現に基づき，デコーダーは翻訳文（翻訳後の単語列）を出力する． エンコーダーとデコーダーは通常，リカレントニューラルネットワーク(RNN)またはその変種によってモデル化される． 例えば，エンコーダーとして10.2節で紹介したBi-LSTM，デコーダーとしてアテンション機構を備えたRNNモデル(Bahdanau *et al*., 2014)は選択肢の一つとなる
[^3]． Marcheggiani *et al*.(2018)では，文章の構文構造情報を取り込んで機械翻訳の性能を向上させるために，10.2節で紹介した同じ戦略がエンコーダーの設計に採用している． 一方でデコーダーは従来モデルと同様，つまりアテンション機構に基づいたRNNモデルを用いている． エンコーダーについては既に10.2節で紹介しているので簡潔に説明することにしよう． エンコーダーでは，最初にBi-LSTMモデルが単語列のエンコードに使用される． このBi-LSTMから出力される表現は，次に依存構造木上のGNNモデルの入力として提供される． GNNモデルの単一のグラフフィルタリング演算は式(10.1)に示した通りである． そして，グラフニューラルネットワークモデルの出力がデコーダーの入力として利用される(Bastings *et al*., 2017)．


[メインページ](../../index.markdown)

[章目次](./chap10.md)

[前の節へ](./subsection_02.md) [次の節へ](./subsection_04.md)

[^3]: 訳注：アテンション機構の重要性が認識されてからは，エンコーダーとデコーダーの両方にセルフ・アテンション機構を用いた**Transformer**が広く利用されている(Vaswani *et al*., 2017)．TransformerはRNNと比べて，系列内の離れたトークン間の関係を効果的に捉えることが可能で，様々なNLPタスクにおいて高いパフォーマンスを発揮している．特に，BERT (Devlin *et al*., 2018)やGPT (Radford *et al*., 2018)などの事前学習モデルはTransformerを基礎としており，その効果を顕著に示している．

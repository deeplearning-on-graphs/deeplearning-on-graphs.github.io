[メインページ](../../index.markdown)

[章目次](./chap10.md)
## 10.3. ニューラル機械翻訳

自然言語処理における重要なタスクの一つに機械翻訳がある． 深層学習の発展に伴い，ニューラルネットワークは機械翻訳のために広く採用されてきた． これらのニューラルネットワークベースに基づくモデルは，ニューラル機械翻訳モデルとよばれ，通常，エンコーダ・デコーダ形式をとる． エンコーダは，翻訳元の言語の単語列を入力として受け取り，その系列内の各単語の表現を出力する． その後，エンコーダからの表現に基づき，デコーダは翻訳文（翻訳後の単語列）を出力する． エンコーダとデコーダは通常，リカレントニューラルネットワーク(RNN)またはその変種によってモデル化される． 例えば，エンコーダとして10.2節で紹介したBi-LSTM，デコーダとしてアテンション機構を備えたRNNモデル(Bahdanau *et al*., 2014)は選択肢の一つとなる
[^3]
． Marcheggiani *et al*.(2018)では，文章の構文構造情報を取り込んで機械翻訳の性能を向上させるために，10.2節で紹介した同じ戦略がエンコーダの設計に採用している． 一方でデコーダは従来モデルと同様，つまりアテンション機構に基づいたRNNモデルを用いている． エンコーダについては既に10.2節で紹介しているので簡潔に説明することにしよう． エンコーダでは，最初にBi-LSTMモデルが単語列のエンコードに使用される． このBi-LSTMから出力される表現は，次に依存構造木上のグラフニューラルネットワークモデルの入力として提供される． グラフニューラルネットワークモデルの単一のグラフフィルタリング演算は式(10.1)に示した通りである． そして，グラフニューラルネットワークモデルの出力がデコーダの入力として利用される(Bastings *et al*., 2017)．


[メインページ](../../index.markdown)

[章目次](./chap10.md)

[前の節へ](./subsection_02.md) [次の節へ](./subsection_04.md)

[^3]: 訳注：アテンション機構の重要性が認識されてからは，エンコーダとデコーダの両方にセルフ・アテンション機構を用いた**Transformer**が広く利用されている(Vaswani *et al*., 2017)．TransformerはRNNと比べて，系列内の離れたトークン間の関係を効果的に捉えることが可能で，様々なNLPタスクにおいて高いパフォーマンスを発揮している．特に，BERT (Devlin *et al*., 2018)やGPT (Radford *et al*., 2018)などの事前学習モデルはTransformerを基礎としており，その効果を顕著に示している．

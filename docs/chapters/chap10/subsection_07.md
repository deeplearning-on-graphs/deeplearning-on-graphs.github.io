[メインページ](../../index.markdown)

[章目次](./chap10.md)
## 10.7. 知識グラフ上のグラフニューラルネットワーク

形式的には，知識グラフ $G = (\symscr{V}, \symscr{E}, \symscr{R})$ はノード集合 $\symscr{V}$ ，エッジ集合 $\symscr{E}$ ，関係集合 $\symscr{R}$ から構成される． ノードは多様な種類のエンティティや属性情報を表し，エッジはそれらのノード間の様々な関係を示している． 具体的には，エッジ $e\in\symscr{E}$ は，エッジの始ノードと終ノードである $s,t\in\symscr{V}$ と，それらの間の関係を表す $r\in R$ から成る3つ組 $(s,r,t)$ として表現できる． このような知識グラフに対して，GNNは，ノード表現を学習・利用し様々な下流タスクを効率的に実行するための手段として拡張されている．下流タスクの具体例を以下に挙げた：

-   **知識グラフの補完**

    -   参考文献：Hamaguchi *et al*.(2017); Schlichtkrull *et al*.(2018); Nathani *et al*.(2019); Shang *et al*.(2019a); Wang *et al*.(2019f)

-   **ノードの重要性推定**

    -   参考文献：Park *et al*.(2019)

-   **エンティティリンキング**

    -   参考文献：Zhang *et al*.(2019b)

-   **異言語間の知識グラフの対応付け**

    -   参考文献：Wang *et al*., 2018c; Xu *et al*., 2019e

知識グラフと単純グラフ(simple graph)の主な違いは，エッジ上の関係情報の有無であり，これは知識グラフのGNNを設計する際に考慮すべき重要な要素である． 本節では，まずGNNが知識グラフにどのように一般化されるかを説明する． そこでは，知識グラフの関係的なエッジを処理する主要な2つの方法が存在する：

1.  グラフフィルタの設計にエッジの関係情報を組み込む

2.  知識グラフ内の関係情報を捉えつつ，無向な単純グラフに変換する

ここでは，知識グラフ補完タスクを例に，知識グラフに対するGNNベースの応用例を説明していく．

### 知識グラフに対するグラフフィルタ

知識グラフに対して，様々なグラフフィルターが設計されており，その中でも代表的なものを紹介する． 式(5.22)で述べたようなGCN-Filterは，以下にすることで知識グラフに適応される(schlichtkrull *et al*., 2018)：

 $$
 \symbf{F}^{(l)}_i = \sum_{r\in\symscr{R}}\sum_{v_j\in\symscr{N}_r(v_i)}\dfrac{1}{\|\symscr{N}_r(v_i)\|}\symbf{F}^{(l-1)}_j\symbf{\Theta}^{(l-1)}_r + \symbf{F}^{(l-1)}_i\symbf{\Theta}^{(l-1)}_0
\tag{10.6} $$
 

ここで， $\symscr{N}\_r(v_i)$ は，関係 $r$ を介してノード $v_i$ に接続する近傍ノードの集合を表しており，次のように定義される：  

$$
 \symscr{N}(v_i) = \left\{v_j\| (v_j,r,v_i) \in \symscr{E}\right\} $$


  式(10.6)では，パラメータ $\symbf{\Theta}^{(l-1)}\_r$ は，同じ関係 $r\in \symscr{R}$ を持つエッジによって共有されている．同様のアイデアはHamaguchi *et al*.(2017)でも言及されている． なお，10.5.2節で説明したEntity-GCNは，式(10.6)のグラフフィルターに影響を受けている．

上記のような異なる関係に対して異なる変換パラメータを学習する代わりに，Shang *et al*.(2019a)では各関係の重要性を捉えるためにスカラー値のスコアを学習している．この設計のグラフフィルタリング操作は次のようになる：

 $$
 \symbf{F}^{(l)}_i = \sum_{r\in\symscr{R}}\sum_{v_j\in\symscr{N}_r(v_i)}\dfrac{1}{\|\symscr{N}_r(v_i)\|}\alpha^{(l)}_r\symbf{F}^{(l-1)}_j\symbf{\Theta}^{(l-1)} + \symbf{F}^{(l-1)}_i\symbf{\Theta}^{(l-1)}_0
\tag{10.7} $$
 

ここで，学習対象である $\alpha^{(l)}\_r$ は，関係 $r$ に対する重要度スコアである．

式(10.6)に含まれるパラメータを減らすために，Vashishth *et al*.(2019)では，各関係に対して**関係埋め込み**が学習される． 具体的には， $l-1$ 層後の関係全体 $\symscr{R}$ の関係埋め込みは $\symbf{Z}^{(l-1)}$ として表され，  $\symbf{Z}^{(l-1)}\_r$ は関係 $r$ の埋め込みである． 関係埋め込みは $l$ 層において次のように更新される：  

$$
 \symbf{Z}^{(l)} = \symbf{Z}^{(l-1)}\symbf{\Theta}^{(l-1)}_{\text{rel}} $$


  ここで， $\symbf{\Theta}^{(l-1)}\_{\text{rel}}$ は学習対象のパラメータである． また， $\symscr{N}(v_i)$ は，ノード $v_i$ とあらゆる関係で接続するノードを含んだ，ノード $v_i$ の近傍ノード集合として定義される． したがって， $\symscr{N}(v_i)$ の $v_i$ の近傍を $(v_j,r)$ で表し，これを用いることで「 $v_j$ は関係 $r$ を介して $v_i$ に接続するノードである」ことを示す． さらに，Vashishth *et al*.(2019)では， $\symscr{E}$ の任意のエッジの"逆エッジ"もエッジとして扱われる． つまり， $(v_i,\,r,\,v_j)\in\symscr{E}$ ならば， $(v_j,\,\hat{r},\,v_i)$ もエッジとして考慮に入れることになる( $\hat{r}$ が $r$ の逆関係のエッジを表す)． このようなエッジも含むようにするため，便宜上 $\symscr{E}$ と $\symscr{R}$ の表記を少し変更し，拡大したエッジ集合と関係集合を表すこととする．

こうして，関係に方向性が加わることになるので， $\operatorname{dir}(r)$ を使って関係 $r$ の方向を示す． 具体的には，全てのオリジナルの関係に対しては $\operatorname{dir}(r)=1$ ，すべての逆関係に対しては $\operatorname{dir}(\hat{r})=-1$ とする． これらの表記を用いて，フィルタリング操作は次のように設計される：

 $$
 \symbf{F}^{(l)}_i = \sum_{(v_j,r)\in\symscr{N}(v_i)}\phi(\symbf{F}^{(l-1)}_j, \symbf{Z}^{(l-1)}_r)\symbf{\Theta}^{(l-1)}_{\operatorname{dir}(r)}
\tag{10.8} $$
 

 $\phi(\cdot,\cdot)$ は，減算や乗算のようなパラメータ化されていない演算を表し， $\symbf{\Theta}^{(l-1)}\_{\operatorname{dir}(r)}$ は同じ $\operatorname{dir}(r)$ 方向を持つ全ての関係で共有される学習対象のパラメータである．

### 知識グラフの単純グラフへの変換

Wang *et al*.(2018c)では，知識グラフ用にグラフフィルタリング操作を設計するのではなく，知識グラフの有向な関係情報を捉えるような単純グラフが生成される． そうすることで，この単純グラフに対して既存のグラフフィルタリング操作を自然に適用することができる．

特定の種類の関係 $r$ を通じて一つのエンティティから別のエンティティへの影響を測定する二つのスコアが提案されている：  

$$

\begin{aligned}
\operatorname{fun}(r) = \dfrac{\text{\#Source_with_r}}{\text{\#Edges_with_r}}\\
\operatorname{ifun}(r) = \dfrac{\text{\#Target_with_r}}{\text{\#Edges_with_r}}
\end{aligned}
$$

  ここで， $\text{\#Edges_with_r}$ は関係 $r$ を持つエッジの総数， $\text{\#Source_with_r}$ は関係 $r$ を持つ「始点エンティティ」のユニーク数， $\text{\#Target_with_r}$ は関係 $r$ を持つ「終点エンティティ」のユニーク数を表している， そして，エンティティ $v_i$ からエンティティ $v_j$ への全体的な影響力は次のように定義される：  

$$
 \symbf{A}_{i,j} = \sum_{(v_i,r,v_j)\in\symscr{E}} \operatorname{ifun}(r) + \sum_{(v_j,r,v_i)\in\symscr{E}}\operatorname{fun}(r) $$


  生成された $\symbf{A}\_{i,j}$ は単純グラフの隣接行列 $\symbf{A}$ の $i,j$ 要素とみなすことができる

### 知識グラフ補完

知識グラフ補完タスクでは，つながっていないエンティティペア組の関係を予測することを目指す． これは知識グラフが通常不完全であったり，新たなエンティティが出現して迅速に発展するという特徴から，重要な課題である． 知識グラフ補完は，具体的には与えられた三つ組 $(s,r,t)$ が実際の関係であるか否かを予測するタスクである． この目標を達成するために，三つ組 $(s,r,t)$ にスコア $f(s,r,t)$ を割り当て，「その三つ組が実際の関係としてあり得るか」を測定する必要がある． 特に，スコアリング関数として採用されるのは**DistMult因子分解**(Yang *et al*., 2014)であり，これは次のように表される：  

$$
 f(s,r,t) = \symbf{F}^{(L)^{T}}_s\symbf{R}_r\symbf{F}^{(L)}_t $$


  ここで， $\symbf{F}^{(L)}\_s$ と $\symbf{F}^{(L)}\_t$ はそれぞれ始点ノード $s$ と終点ノード $t$ の表現であり，それらは $L$ 層のフィルタリング層を持つGNNによって学習される． また， $\symbf{R}\_r$ は関係 $r$ に対応する対角行列であり，学習過程で最適化される． このモデルはネガティブサンプリングを備えた交差エントロピー損失関数を用いて学習できる． 特に，観測されたエッジサンプル $e\in\symscr{E}$ ごとに，始点ノード $s$ または終点ノード $t$ を別のエンティティとランダムに置き換えることにより $k$ 個のネガティブサンプル(負例)が生成される． 実際に観測されたサンプル(正例)とネガティブサンプル(負例)を用いて，最適化される交差エントロピー損失は次のように表現できる：  

$$
 \symscr{L} = -\dfrac{1}{(1+k)\|\symscr{E}\|}\sum_{(s,r,o,y)\in\symscr{T}}y\log \sigma(f(s,r,o)) + (1-y)\log(1-\sigma(f(s,r,o))) $$


  ここで， $\symscr{T}$ は $\symscr{E}$ で観測された正のサンプルとランダムに生成された負のサンプルの集合を表しており， $y$ は観測サンプルに対しては $1$ に，負のサンプルに対しては $0$ に設定される指示関数である．


[メインページ](../../index.markdown)

[章目次](./chap10.md)

[前の節へ](./subsection_06.md) [次の節へ](./subsection_08.md)



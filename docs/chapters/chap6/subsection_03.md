[メインページ](../../index.markdown)

[章目次](./chap6.md)
## 6.3. 敵対的攻撃に対する防御

グラフデータに対する敵対的な攻撃を防ぐために，様々な防御手法が提案されている． これらの防御手法は主に4つに分類することできる．

1.  **グラフの敵対的学習：**モデルのロバスト性を向上させるために，学習手順に敵対的なサンプルを組み込む手法．

2.  **グラフの純化(Purification)：**敵対的攻撃を検出し，攻撃されたグラフからそれらを除去してクリーンなグラフを生成しようと試みる手法．

3.  **グラフアテンション防御：**学習段階で敵対的攻撃を特定し，モデルの学習中にそれらの攻撃への注目（アテンション）を低く設定する手法．

4.  **グラフ構造学習：**GNNモデルを訓練しながら，同時に攻撃されたグラフからクリーンなグラフを学習することを目指す手法．

ここからは，上記の各分類別における代表的な手法をいくつか紹介していこう．

### グラフの敵対的学習

「敵対的学習」という概念は(Goodfellow *et al*., 2014b)，敵対的なサンプルをモデルの学習段階に取り入れることにより，モデルのロバスト性（堅牢性）を向上させるものである． この方法は，画像分野での頑健な深層モデルの訓練において，その効果が実証されている(Goodfellow *et al*., 2014b)． 敵対的学習は通常，次の2つのステップで行われる．

1.  敵対的攻撃の実行

2.  敵対的攻撃によって得られたサンプルを用いたモデルの学習

グラフ領域(graph domain)では，敵対者はグラフ構造やノードの特徴量を変更することが許されている． ゆえに，グラフ領域での敵対的学習は，取り入れる敵対的攻撃の種類に従い， 以下のように分類することができる：

1.  グラフ構造 $\symbf{A}$ に対する敵対的攻撃のみ．

2.  ノード特徴量 $\symbf{F}$ に対する敵対的攻撃のみ．

3.  グラフ構造 $\symbf{A}$ およびノード特徴量 $\symbf{F}$ の両方への敵対的攻撃．

次に，代表的なグラフ領域での敵対的学習の技術を紹介していく．

#### グラフ構造を用いた敵対的学習

Dai *et al*. (2018)では，直感的でシンプルなグラフ敵対的学習法が提案されている． この手法における学習段階では，入力グラフ内のエッジをランダムに削除することで「敵対的に攻撃されたグラフ」が生成される． この方法はシンプルでロバスト性を向上させる効果は限定的であるが，グラフ構造データに対する敵対的学習の先駆けとなる手法である． その後，PGDトポロジー攻撃に基づく様々なグラフ敵対的学習法が提案された． 具体的には，この敵対的学習は以下のmin-max最適化問題として定式化することができる：

 $$
 \min_{\symbf{\Theta}}\max_{\symbf{s}\in \symbf{S}} - \symscr{L}(\symbf{s};\,\symbf{\Theta})

\tag{6.13} $$
 

ここで，目的関数 $\mathscr{L}(\symbf{s};\,\symbf{\Theta})$ は，全体の学習データ集合 $\mathscr{V}\_l$ について，式(6.4)と同様に定義されている：  

$$

\begin{aligned}
%TODO: 「\min_{\symbf{s}}」を除くか迷う．(6.4)と合わせるためには，\minをつけるか決める必要がある．
    &\mathcal{L}(\symbf{s}) = \sum_{v_i\in \mathcal{V}_l}\ell (f_{\mathrm{GNN}}(\mathcal{G}';\;\symbf{\Theta})_i,\,y_i)\\
    &\text{subject to } \quad \|\symbf{s}\|_0\leq \Delta,\quad\symbf{s}\in \left\{0,\,1\right\}^{N(N-1)/2}
\end{aligned}
$$

  式(6.13)のmin-max問題を解くことは，（例えば，PGDトポロジー攻撃アルゴリズムによって）生成された敵対的グラフ構造の摂動下で「学習データにおける損失を最小化」することを意味する．

最小化問題と最大化問題は交互に処理されることになる．最大化問題では，6.2.2節で紹介したPGDアルゴリズムを用いて解くことができる．その結果，連続的な解 $\symbf{s}$ が得られる． 二値でない隣接行列 $\symbf{A}$ は，連続的な $\symbf{s}$ に従って生成される． この敵対的グラフは，分類モデルのパラメータ $\symbf{\Theta}$ の学習プロセス（最小化問題）において，攻撃を受けた状況を再現する役割を果たす．

#### ノード特徴量を用いた敵対的学習

GraphAT (Feng *et al*., 2019a)という手法では，ノード特徴量に基づいた敵対的サンプルを分類モデルの学習プロセスに組み込んでいる． ここでの敵対的サンプルは，攻撃を受けていないノード特徴量サンプルを摂動させることによって生成される． この摂動は，隣接するノードが異なるラベルに分類されやすくなるように行われる．

グラフニューラルネットワークモデルにおける重要な仮定として，「隣接するノードは互いに似ている傾向がある」というものがある [^8]
そのため，ノード特徴量への敵対的な攻撃は，モデルの誤りを誘発する可能性を高めることになる． こうして生成された敵対的サンプルは，"正則化項"の形で学習プロセスで活用される． 具体的には，本手法の一連の学習プロセスは以下のmin-max最適化問題として表現することができる：

 

$$

\begin{aligned}
        \min_{\symbf{\Theta}}\symscr{L}_{\text{train}} + \beta\sum_{v_i\in\symscr{V}}\sum_{v_j\in\symscr{N}(v_i)}&d(f_{\text{GNN}}(\symbf{A},\symbf{F}\star\symbf{r}^{g}_i;\,\symbf{\Theta})_i,f_{\text{GNN}}(\symbf{A},\symbf{F};\,\symbf{\Theta})_j);\nonumber\\
        \symbf{r}^{g}_i=\underset{\symbf{r}_i,\|\symbf{r}_i\|\leq \varepsilon}{\operatorname{argmax}} \sum_{v_j\in\symscr{N}(v_i)}&d(f_{\text{GNN}}(\symbf{A},\symbf{F}\star\symbf{r}_i;\,\symbf{\Theta})_i,f_{\text{GNN}}(\symbf{A},\symbf{F};\,\symbf{\Theta})_j);
        
\end{aligned}
\tag{6.14}
$$

 

最大化問題では，つながりのあるノード間の特徴量が似ているというパターンを崩すような，敵対的ノード特徴量を生成する [^9]
一方，最小化問題では，パラメータ $\symbf{\Theta}$ を学習する． ただしこの学習は，訓練誤差を小さく保つだけでなく，二項目に追加された正則化項を介して敵対的サンプルとそれらの近傍ノードとの間の特徴量の類似性を保つことを目指して行われる．

式(6.14)で定義される $\symscr{L}\_{\text{train}}$ は，式(5.49)で定義された損失関数で， $\symbf{r}\_i\in\mathbb{R}^{1\times d}$ は各行（グラフ内の各ノード）に対する敵対的ベクトルである． また， $\symbf{F}\star\symbf{r}\_i$ という操作は， $\symbf{r}\_i$ を $\symbf{F}$ の $i$ 番目の行への和を意味し，これはノード $v_i$ の特徴量に敵対的ノイズを加えることに相当する． さらに， $f\_{\text{GNN}}(\symbf{A},\symbf{F}\star\symbf{r}^{g}\_i;\,\symbf{\Theta})\_i$ は $f\_{\text{GNN}}(\symbf{A},\symbf{F}\star\symbf{r}^{g}\_i;\,\symbf{\Theta})$ の $i$ 番目の行を表し，これはノード $v_i$ に対する予測結果（出力結果）を表している． 関数 $d(\cdot,\,\cdot)$ はKLダイバージェンス(Joyce, 2011)であり，予測結果の間の差異を測定する． これらの最小化問題と最大化問題は交互に処理される．

#### グラフ構造とノード特徴量の両方を用いた敵対的学習

グラフ構造 $\symbf{A}$ とノード特徴量 $\symbf{F}$ が持つ"離散性"に起因する課題に対処するためのグラフの敵対的学習が提案されている． その手法では，最初のグラフフィルタリング層 $\symbf{F}^{(1)}$ の"連続的"な出力を調整することでこの課題に取り組んでいる(Jin and Zhang, n.d.)． 具体的には，最初の隠れ表現 $\symbf{F}^{(1)}$ に対する敵対的攻撃を生成し，それらをモデルの学習段階に組み込むことになる． この手法の一連の学習プロセスは以下のmin-max最適化問題としてモデル化することができる：

 $$
 
\tag{6.15}
    \min_{\symbf{\Theta}}\max_{\xi\in D}\symscr{L}_{\text{train}}\left(\symbf{A},\symbf{F}^{(1)} + \xi;\,\symbf{\Theta}\right) $$
 

この式において，最大化問題では最初の層の隠れ表現 $\symbf{F}^{(1)}$ に微小な敵対的摂動を生成する． これは間接的にグラフ構造 $\symbf{A}$ とノード特徴量 $\symbf{F}$ への摂動を表現していることになる． 一方，最小化問題では生成された摂動を学習プロセスに組み込みながらモデルのパラメータを学習する．  $\xi$ は学習させるべき敵対的ノイズであり， $D$ はそのノイズの制約領域を示しており，以下のように定義される：  

$$
 D = \left\{\xi;\,\|\xi\|_2\leq\Delta\right\} $$


  ここで， $\xi_i$ は $\xi$ の $i$ 番目の行を示し， $\Delta$ は予め定義された制約値である．また，式(6.15)において， $\symscr{L}\_{\text{train}}\left(\symbf{A},\symbf{F}^{(1)} + \xi;\,\symbf{\Theta}\right)$ は摂動が加えられた隠れ表現 $\symbf{F}^{(1)}+\xi$ に基づいている点を除けば，式(5.49)と同様の損失関数を示すために使用されている． 他の敵対的学習手法と同様に，本手法でも，最小化問題と最大化問題は交互に処理されることになる．

### グラフの純化

グラフ構造に対する敵対的攻撃を防ぐために，**グラフの純化**に基づく防御技術が開発されている． この手法は，モデルの学習前に，与えられたグラフ内の敵対的な攻撃を特定し，それらを除去することを試みる． したがって，グラフ純化に関係する手法の多くは，「グラフの前処理」とみなすことができる． 本節では，こうしたグラフ純化に基づく防衛技術を二つ紹介していく．

#### 特徴量の類似度が低いエッジの除去によるグラフ純化

様々な敵対的攻撃の手法（例えば，NettackやIG-FGSM）を用いた実験結果から，これらの手法は特徴量が大きく異なるノード間にエッジを追加する傾向があることがわかっている (Wu *et al*., 2019;  Jin *et al*., 2020a)．同じく，エッジを削除する場合には，これらの攻撃手法は類似した特徴量を持つノード間のエッジを取り除く傾向がある． これらの観測結果に基に，Wu *et al*.(2019) では，特徴量が大きく異なるノード同士を接続するエッジを取り除くというシンプルかつ効率的なアプローチが提案されている． 具体的には，ノード特徴量の類似性を測るスコアリング関数が導入されている． 例えば二値の特徴量に対しては，Jaccard係数(Tan *et al*., 2016)によるスコアリング関数が採用されており， このスコアが一定の閾値を下回るエッジをグラフから取り除く． 以上の処理で純化されたグラフが得られ，通常のグラフニューラルネットワークモデルの学習に利用される．

#### 隣接行列の低ランク近似によるグラフの純化

実証研究が行われ，Nettackによって生成される敵対的な摂動が分析された(Entezari *et al*., 2020; Jin *et al*., 2020a)． その結果，Nettackはグラフ構造を微小に変化させることで，対応する隣接行列のランク [^10]
さらには，隣接行列の小さい特異値(singular value)の個数が増加することが指摘されている．

そこで，Entezari *et al*.(2020)で，グラフ構造に追加された敵対的な摂動を取り除くための，特異値分解(SVD; Singular Value Decomposition)をベースとした前処理方法が提案されている． 具体的には，グラフの隣接行列 $\symbf{A}$ が与えられたとき，それを特異値分解する．その後，特異値の大きい順から $k$ 個の特異値だけを保持し，それを用いて隣接行列を再構成（近似）する． 再構成された隣接行列は，純化されたグラフ構造として扱われ，グラフニューラルネットワークモデルの学習に利用される．

### グラフアテンション防御

グラフの純化に基づく方法がグラフから敵対的攻撃を取り除くのに対し，**グラフアテンション**に基づく方法では，敵対的攻撃によって影響を受けたグラフ内のノードやエッジに対する注目度を低く抑えるように学習することを目的としている． グラフアテンションに基づく防御技術は，通常，入力から最終結果までを一連の流れ通して扱う形式で実行される． つまり，これらの防御技術はグラフアテンション機構を，グラフニューラルネットワークモデルの一部として組み込んでいる． 本節では，アテンション機構に基づく防御技術の手法を二つ紹介していく．

#### RGCN：正規分布による隠れ表現のモデル化

GNNモデルのロバスト性を強化するため，Zhu *et al*.(2019a)では，確定的なベクトルの代わりに多変量正規分布を用いた隠れ表現のモデリングを採用している． 敵対的攻撃はグラフ構造に摂動を与え，それがノード表現に不自然な変化を引き起こす． これらの敵対的な影響に対して，確定的（非確率的）なベクトルに基づく隠れ表現は自己調整する能力を持たないのに対し，正規分布に基づく隠れ表現は敵対的な攻撃によって生じる影響を吸収し，結果としてよりロバストな表現を生み出すことができる．

さらに，グラフ全体に敵対的な影響が伝播するのを防ぐために，「分散に基づくアテンション機構」が導入されている． 敵対的攻撃の影響を受けたノードの隠れ表現は，通常，大きい分散値を持つ傾向にある． これは敵対的攻撃が特徴量が大幅に異なるノードや，異なるコミュニティのノードを接続しようとするためである． したがって，近傍情報を集約してノード特徴量を更新する際，大きな分散値を持つ近傍ノードへの注目度（アテンション）は低く設定される．これにより，敵対的な影響が全体に広がるのを防ぐ． 次に，上で述べた直感に基づいて構築されたグラフフィルターである**RGCNフィルタ**の詳細を説明する．

RGCNフィルタは，式(5.22)に記述されているGCNフィルタを基礎として構築される． 詳細な説明を行うため，以下に式(5.22)を再度示す：  

$$
 \symbf{F}^{\prime}_i = \sum_{v_j\in\symscr{N}(v_i)\cup \left\{v_i\right\}}\dfrac{1}{\sqrt{\tilde{\symbf{d}}_i\tilde{\symbf{d}}_j}}\symbf{F}_j\symbf{\Theta} $$


  ここで， $\tilde{\symbf{d}}\_i = \tilde{\symbf{D}}\_{i,i}$ である． RGCNフィルタでは，確定的なベクトルの代わりに正規分布によってノード表現をモデル化する． ノード $v_i$ について，その表現は次のように表される：  

$$
 \symbf{F}_i \sim \symscr{N}(\symbf{\mu}_i,\,\operatorname{diag}(\symbf{\sigma}_i)) $$


  ここで， $\symbf{\mu}\_i\in\mathbb{R}^d$ はノード表現の平均， $\mathrm{diag}(\symbf{\sigma}\_i)\in\mathbb{R}^{d\times d}$ は（各ノードが持つ分散を対角成分においた）ノード表現の対角分散行列である．

ノード表現を更新する際，表現の平均と分散に対する2つの集約プロセスがある． さらに，敵対的攻撃の効果がグラフ全体へ伝播するのを防ぐために，ノード表現の分散に対するアテンション機構が導入されている． 具体的には，分散が大きいノードには，より小さいアテンションスコアが割り当てられることになる． ノード $v_i$ のアテンションスコアは，以下のような微分可能な指数関数を通じてモデル化される：  

$$
 \symbf{a}_i = \exp (-\gamma\symbf{\sigma}_i) $$


  ここで， $\gamma$ はハイパーパラメータである．

以上，正規分布に基づいたノード表現法とアテンションスコアの定義により，ノード $v_i$ に関する表現の更新プロセスは次のように記述することができる：  

$$
 \symbf{F}^{\prime}_i\sim \symscr{N}(\symbf{\mu}^{\prime}_i,\,\operatorname{diag}(\symbf{\sigma}^{\prime}_i)) $$


  ここで， $\symbf{\mu}^{\prime}\_i$ と $\symbf{\sigma}^{\prime}\_i$ は以下のように与えられる．  

$$

\begin{aligned}
    \symbf{\mu}^{\prime}_i &= \alpha\left\{\sum_{v_j\in\symscr{N}(v_i)\cup\left\{v_i\right\}}\dfrac{1}{\sqrt{\tilde{\symbf{d}}_i\tilde{\symbf{d}}_j}}\left(\symbf{\mu}_j\odot\symbf{a}_j\right)\symbf{\Theta}_{\mu}\right\}\\
    \symbf{\sigma}^{\prime}_i &= \alpha \left\{\sum_{v_j\in\symscr{N}(v_i)\cup\left\{v_i\right\}}\dfrac{1}{\tilde{\symbf{d}_i}\tilde{\symbf{d}}_j}\left(\symbf{\sigma}_j\odot\symbf{a}_j\odot\symbf{a}_j\right)\symbf{\Theta}_{\sigma}\right\}
\end{aligned}
$$

  この式において， $\alpha$ は非線形の活性化関数， $\odot$ はアダマール積の演算子を表している．  $\symbf{\Theta}\_{\mu}$ と $\symbf{\Theta}\_{\sigma}$ はそれぞれ，平均と分散の集約情報を変換するための学習対象のパラメータである．

#### PA-GNN：クリーンなグラフから作られる敵対的エッジを活用した学習

RGCNでは影響を受けたノードに罰則を科したのに対し，PA-GNN (Tang *et al*., 2019)ではグラフ全体にわたる敵対的な効果の伝播を防ぐために，「敵対的なエッジに罰則を科す」ことを目的としている． 具体的には，「敵対的なエッジに重要度(アテンションスコア)を低く割り当てるアテンション機構」を学習することを目指している． しかし，我々は通常，どれが敵対的なエッジ（攻撃が行われて影響を受けたエッジ）であるかに関する情報を持っていない． そのためPA-GNNでは，クリーンなグラフ（敵対的な攻撃がなされていないグラフ）への敵対的攻撃で得た知識を，望ましいアテンションスコアの学習のための教師信号として機能させることを提案している．

PA-GNNモデルは，式(5.27)で記述されているグラフアテンションネットワークに基づいて構築されており，以下のように書き表すことができる：

 $$
 
\tag{6.16}
\symbf{F}^{\prime}_i = \sum_{v_j\in\symscr{N}(v_i)\cup\left\{v_i\right\}}a_{ij}\symbf{F}_j\symbf{\Theta} $$
 

ここで $a_{ij}$ は，ノード $v_j$ からノード $v_i$ へ情報を集約する際に使用されるエッジ $e_{ij}$ のアテンションスコアを示している． 直感的には，敵対的なエッジのアテンションスコアが小さければ，敵対的な影響が伝播するのを防げるはずである．

仮に敵対的なエッジ集合がわかっているとしよう．それを $\symscr{E}\_{ad}$ と表すことにする． すると，残りの「クリーン」なエッジ集合は $\symscr{E}/\symscr{E}\_{ad}$ と表すことができる． 敵対的なエッジのアテンションスコアが小さくなるように保証するために，敵対的なエッジに罰則を科す次の損失関数を追加する：

 $$
 \symscr{L}_{\text{dist}} = -\min\left(\eta,\underset{\underset{1\leq l\leq L}{e_{ij}\in\symscr{E}/\symscr{E}_{ad}}}{\mathbb{E}} a^{(l)}_{ij} - \underset{\underset{1\leq l\leq L}{e_{ij}\in\symscr{E}_{ad}}}{\mathbb{E}} a^{(l)}_{ij}\right)
%TODO: 集合の差集合を本書全体で統一させる必要あり． $$
  ここで， $a^{(l)}_{ij}$ は $l$ 番目のグラフフィルタリング層におけるエッジ $e_{ij}$ に割り当てられたアテンションスコアである． また， $L$ はモデル内のグラフフィルタリング層の総数， $\eta$ は「2つの期待値の間の差」を調整するハイパーパラメータである． アテンションスコアの期待値は，全フィルタリング層にわたる以下の平均値によって推定される：  

$$

\begin{aligned}
    \underset{\underset{1\leq l\leq L}{e_{ij}\in\symscr{E}/\symscr{E}_{ad}}}{\mathbb{E}} a^{(l)}_{ij} &= \dfrac{1}{L\|\symscr{E}/\symscr{E}_{ad}\|}\sum^{L}_{l=1}\sum_{e_{ij}\in\symscr{E}/\symscr{E}_{ad}}a^{(l)}_{ij}\\
    \underset{\underset{1\leq l\leq L}{e_{ij}\in\symscr{E}_{ad}}}{\mathbb{E}}a^{(l)}_{ij} &= \dfrac{1}{L\|\symscr{E}_{ad}\|}\sum^{L}_{l=1}\sum_{e_j\in\symscr{E}_{ad}}a^{(l)}_{ij}
\end{aligned}
$$

  ここで， $\|\cdot\|$ は集合の要素数（cardinality）を表している．敵対的なエッジへのアテンションスコアを低く設定しながら分類モデルを学習するためには，損失関数 $\symscr{L}\_{\text{dist}}$ と式(5.49)におけるノード分類の損失関数 $\symscr{L}\_{\text{train}}$ を組み合わせる．具体的には以下のようになる：

 $$
 
\tag{6.17}
\min_{\symbf{\Theta}}\symscr{L} = \min_{\symbf{\Theta}} (\symscr{L}_{\text{train}} + \lambda\symscr{L}_{\text{dist}}) $$
 

ここで， $\lambda$ は2つの損失関数の間の重要性を調整するハイパーパラメータである．

ここまでは，敵対的エッジの集合 $\symscr{E}\_{ad}$ は既知であると仮定していたが，これは現実的ではない． そのため，式(6.17)を直接的に定式化し最適化するのではなく，既知の敵対的エッジを持つグラフからその敵対的エッジに低いアテンションスコアを割り当てる能力を学習し，それを本来対象とするグラフに活用することを考える． 既知の敵対的エッジを持つグラフを得るためには，与えられた対象グラフと同じ種類のデータや同じカテゴリの情報を持ったグラフを集め [^11]
それから，これらの攻撃されたグラフから，敵対的エッジに対する低いアテンションスコアの割り当て能力を獲得し，それを与えられた対象グラフに適用する．

ここからはPA-GNNの全体的なフレームワークについて簡単に議論していこう．その後，アテンション機構の学習プロセスとその能力を対象グラフに伝達するプロセスを詳細に説明していく． はじめに，図6.1に示すように， $K$ 個のクリーンなグラフ集合 $\left\\{\symscr{G}\_1,\cdots,\,\symscr{G}\_K\right\\}$ が与えられた場合，metattackのような既存の攻撃手法を使って各グラフに対する敵対的エッジ集合 $\symscr{E}^{i}\_{ad}$ を生成する． 次に，各グラフのノード集合 $\symscr{V}^i$ を，訓練用のノード集合 $\symscr{V}^{i}\_l$ とテスト用のノード集合 $\symscr{V}^{i}\_u$ に分割し，各グラフに対して式(6.17)の損失関数の最適化を試みる．具体的には，グラフ $\symscr{G}\_i$ に対応する損失関数を $\symscr{L}\_i$ と表す． そして，メタ最適化アルゴリズム**MAML** (Finn *et al*., 2017)の考えを取り入れて，全てのグラフが同一の初期化パラメータ $\symbf{\Theta}$ を共有するという設定の下，各グラフの特定の学習タスクに簡潔に適応できるようなパラメータ $\symbf{\Theta}$ を学習することを目指す． 図6.1に示すように，全グラフが共有する理想的な初期化パラメータ $\symbf{\Theta}$ はメタ最適化(meta-optimization)を通じて学習され，これについては後述する．

<figure>

<img src="./fig/fig6_1.png" width="100%"/>

<figcaption>図6.1 PA-GNNの全体的なフレームワーク</figcaption>

</figure>

これらの共有パラメータ $\symbf{\Theta}$ は，敵対的エッジに低いアテンションスコアを割り当てる能力を持つと考えられる． この能力を与えられた対象グラフ $\symscr{G}$ に伝達するために，共有パラメータ $\symbf{\Theta}$ を初期化パラメータとして用い，グラフニューラルネットワークモデルをグラフ $\symscr{G}$ を使って学習（ファインチューニング）する． その結果得られたファインチューニングされたパラメータは $\symbf{\Theta}\_G$ として表される． 以上がPA-GNNの全体的なフレームワークについての説明である．次に，MAMLから採用したメタ最適化アルゴリズムを用いて，最適な共有パラメータ $\symbf{\Theta}$ がどのように学習されるかを具体的に説明していく．

パラメータの最適化プロセスはまず，以下のように勾配降下法を使用してパラメータ $\symbf{\Theta}$ を各グラフ $\symscr{G}\_i$ を使って微調整(更新)する：  

$$
 \symbf{\Theta}^{\prime}_i = \symbf{\Theta} - \alpha\nabla_{\symbf{\Theta}}\symscr{L}^{\text{tr}}_i(\symbf{\Theta}) $$


  ここで， $\symbf{\Theta}^{\prime}\_i$ はグラフ $\symbf{G}\_i$ の学習タスクに対する特定のパラメータであり，  $\symscr{L}^{\text{tr}}\_i$ は対応する学習ノード集合 $\symscr{V}^{i}\_l$ で評価される式(6.17)の損失関数を表している． その後，すべてのグラフのテストノード集合 $\left\\{\symscr{V}\_u^{1},\dots,\symscr{V}\_u^{K}\right\\}$ を用いて共有パラメータ $\symbf{\Theta}$ を更新し，学習された各クラス分類器がそれらのグラフでうまく機能するようにする． 以上より，メタ最適化の目的関数は次のようにまとめることができる：

 $$
 \min_{\symbf{\Theta}}\sum^{K}_{i=1}\symscr{L}^{\text{te}}_i(\symbf{\Theta}^{\prime}_i)=
    \min_{\symbf{\Theta}}\sum^{K}_{i=1}\symscr{L}^{\text{te}}_i\left(\theta - \alpha\nabla_{\symbf{\Theta}}\symscr{L}^{\text{tr}}_i(\symbf{\Theta})\right) $$
 

ここで， $\symscr{L}^{\text{te}}\_i\left(\symbf{\Theta}^{\prime}\_i\right)$ は対応するテストノード集合 $\symscr{V}^{i}\_u$ で評価された式(6.17)の損失関数を表している． そして共有パラメータ $\symbf{\Theta}$ は，以下のSGD(確率的勾配降下法)を使用して更新することができる：  

$$
 \symbf{\Theta} \leftarrow \symbf{\Theta} - \beta\nabla_{\symbf{\Theta}}\sum^{K}_{i=1}\symscr{L}^{\text{te}}_i\left(\theta - \alpha\nabla_{\symbf{\Theta}}\symscr{L}^{\text{tr}}_i(\symbf{\Theta})\right) $$


 

この共有パラメータ $\symbf{\Theta}$ は，一度学習されると，与えられたグラフ $\symscr{G}$ 上の(ファインチューニングを目的とした)学習タスクの初期化パラメータとして使われます．

### グラフ構造学習

6.3.2節では，グラフ純化に基づく防御技術を紹介した． これらの技術の多くは、敵対的攻撃を識別してから、GNNモデルを訓練前に攻撃されたグラフからこれらの攻撃を除去するという手順を踏むものであり，純化ステップとモデル学習ステップの2つの段階から通常成り立っている． このような二段階の戦略の下では，純化した後のグラフが下流タスクのモデルパラメータを学習にとって最適とは限らない可能性がある．

そこで (Jin *et al*., 2020b)では，グラフ構造を純化し，モデルパラメータを同時に学習する全体的に一貫した方法が提案されており，これによりロバストなグラフニューラルネットワークモデルを学習することができる． 6.3.2節では，敵対的攻撃は通常，異なるノード特徴量を持つノード同士の間にエッジを追加し，隣接行列のランクを増加させる傾向があることを説明した． ゆえに，Jin *et al*.(2020b)で提案されたPro-GNNは，敵対的攻撃による影響を減らすために，接続されたノード間での特徴量の滑らかさを保持しながらも低ランク性を維持し，かつ元の隣接行列 $\symbf{A}$ に近い新しい隣接行列 $\symbf{S}$ の学習を行うことを目指している． 具体的には，純化された隣接行列 $\symbf{S}$ とモデルのパラメータ $\symbf{\Theta}$ は，次の最適化問題を解くことによって学習することができる：

 $$
 
\tag{6.18}
\min_{\symbf{\Theta},\symbf{S}}\symscr{L}_{\text{train}}(\symbf{S},\,\symbf{F};\,\symbf{\Theta}) + \|\symbf{A} - \symbf{S}\|^2_F + \beta_1\|\symbf{S}\|_1 + \beta_2\|\symbf{S}\|_{\ast} + \beta_3\cdot\mathrm{tr}(\symbf{F}^{T}\symbf{L}\symbf{F}) $$
 

ここで， $\|\symbf{A}-\symbf{S}\|^{2}\_F$ は学習された行列 $\symbf{S}$ が元の隣接行列 $\symbf{A}$ に近づくことを保証するためのものである． また，学習された隣接行列の $L_1$ ノルムである $\|\symbf{S}\|\_1$ の項により，学習された行列 $\symbf{S}$ が疎になることが可能となる．  $\|\symbf{S}\|\_{\ast}$ は，学習された行列 $\symbf{S}$ が低ランクであることを保証するための核ノルム(nuclear norm)であり [^12]
このとき特徴量行列 $\symbf{F}$ は変化しないものとして扱われているので， $\mathrm{tr}(\symbf{F}^{T}\symbf{L}\symbf{F})$ の項は， $\symbf{S}$ を元にしたラプラシアン行列 $\symbf{L}$ が，接続されたノード間での特徴量が滑らかになるように調整される．  $\beta_1,\,\beta_2,\,\beta_3$ はこれらの項間のバランスを制御するハイパーパラメータである．

行列 $\symbf{S}$ とモデルパラメータ $\symbf{\Theta}$ は，以下のように交互に最適化することができる：

・ $\symbf{\Theta}$ の更新：

:   行列 $\symbf{S}$ を固定し，式(6.18)の $\symbf{S}$ と無関係な項を削除する．すると最適化問題は次のように最定式化される：  

$$
 \min_{\symbf{\Theta}} \symscr{L}_{\text{train}}(\symbf{S},\,\symbf{F};\,\symbf{\Theta}) $$


 

・ $\symbf{S}$ の更新：

:   モデルパラメータ $\symbf{\Theta}$ を固定し，以下の最適化問題を解くことで行列 $\symbf{S}$ を最適化する：

     

$$
 \min_{\symbf{S}}\symscr{L}_{\text{train}}(\symbf{S},\,\symbf{F};\,\symbf{\Theta}) + \|\symbf{A} - \symbf{S}\|^2_F + \alpha\|\symbf{S}\|_1 + \beta\|\symbf{S}\|_{\ast} + \lambda\cdot\mathrm{tr}(\symbf{F}^{T}\symbf{L}\symbf{F}) $$


 


[メインページ](../../index.markdown)

[章目次](./chap6.md)

[前の節へ](./subsection_02.md) [次の節へ](./subsection_04.md)

[^8]: 訳注：ここで言う「似ている」とは，ノードの特徴量や，ノードが持つラベル，さらにはノードがグラフ内で占める位置などの構造的な類似性が一致あるいは近いことを指す．．
[^9]: 訳注：「特徴量が似ている」とは，そのノードが持つ生の特徴量だけでなく，「モデルのパラメータを用いて変換された特徴量（すなわち，モデルが学習プロセスで獲得する特徴量表現）が似ている」という意味も持つ．．
[^10]: 訳注：行列のランクとは，行列における線形独立な行または列の最大数のことを指す．言い換えると，ランクはその行列が表現できる次元数，すなわちその行列が表す空間の次元を示す．ランクが大きいほど，行列はより多くの次元の情報を含んでいると言える．を高める傾向があることが明らかになった．
[^11]: 訳注：例えば，対象グラフが「データマイニング分野の論文の引用ネットワーク」である場合，類似したグラフは「物理学分野の引用ネットワーク」といった具合である．，metattackのような既存の敵対的攻撃を実行して攻撃されたグラフを生成する．
[^12]: 訳注：ある行列 $\symbf{A}$ の核ノルムは，その行列の特異値（つまり，その行列を特異値分解したときに得られる値）の和として定義される： $\|\symbf{A}\|\_{\ast} = \sum_{i=1}\sigma_i$ ．ここで， $\sigma_i$ は $\symbf{A}$ の第 $i$ 特異値を表す．つまり，特異値の個数が多く，それらの和が大きい行列は高ランク（つまり，情報が非常に複雑または雑多である）とみなされ，その核ノルムも大きくなる．逆に，特異値の個数が少なく，それらの和が小さい行列は低ランク（つまり，情報が簡素または一貫している）とみなされ，その核ノルムも小さくなる．， $\mathrm{tr}(\symbf{F}^{T}\symbf{L}\symbf{F})$ は接続されたノード間の特徴量の滑らかさを保つためのものである．

[メインページ](../../index.markdown)

[章目次](./chap12.md)
## 12.2. Webデータマイニング

SNSやECサイトなどの多数のWebアプリケーションでは，膨大な量のデータが日々生成されている． Webデータマイニングとは，この種のデータが持つパターンを見つけ出すために，データマイニング技術が適用されるプロセスを指す． 本節では，Webデータマイニングの代表的な2つのタスク，すなわちソーシャルネットワーク分析(Social Network Analysis)と推薦システム(Recommender system)の発展に，GNNがどのように貢献しているかを示す．

### ソーシャルネットワーク分析

ユーザ間の関係や相互作用を特徴付けるソーシャルネットワークは，Web全体，特にソーシャルメディアにおいて広く存在している． ソーシャルネットワークはそのままグラフとしてモデル化することができ，この場合，ネットワーク内のユーザがノード，関係や相互作用がエッジとなる． グラフニューラルネットワークは，社会的影響の予測(Qiu *et al*., 2018a)，政治的立場の識別(Li and Goldwasser, 2019)，社会的関係性の表現学習(Wang *et al*., 2019a)など，ソーシャルネットワーク上の様々なタスクの実行に採用されてきている． ここからは，これらのタスクの一部の詳細を説明していく．

#### 社会的影響の予測

ソーシャルネットワークでは，個々の感情や意見，行動，決定は，他者から影響を受ける． この現象は一般的に**社会的影響**(social influence)と呼ばれ，（学校や職場のような）物理的ソーシャルネットワークや，（SNSのような）オンラインソーシャルネットワークにおいて広く観察される． こうした社会的影響の調査は，広告戦略の最適化やパーソナライズされたレコメンデーションの実行に際して重要である． Qiu *et al*.(2018a)の研究では，ソーシャルネットワーク内のユーザに対する局所的な社会的影響
[^1]
を予測するためにGNNが採用されている． 具体的には，ある対象ユーザの局所的近傍ユーザと，その近傍ユーザのアクション(action)が与えられたとき，対象ユーザが将来そのアクションを取るかどうかを予測することが目標である． 例えば，Twitter上では，あるユーザが特定のトピックに関する投稿をリツイート（アクション）するかどうかを予測するタスクがある．この予測は，そのユーザと何らかの形でつながりのある他ユーザ（局所的近傍ユーザ）のアクション状況（リツイートを行ったかどうか）の情報を基に行われる．

ソーシャルネットワーク内のユーザ同士の関係は，グラフ $\symscr{G} = \left\\{\symscr{V},\symscr{E}\right\\}$ として表現することができる． ここで， $\symscr{V}$ はソーシャルネットワーク内のユーザ集合を表し， $\symscr{E}$ はユーザ間の関係を表す． そして，各ノード $v_i\in\symscr{V}$ について，その局所近傍は，ノード $v_i$ から $r$ 次近傍内の全ノードを含む $\symscr{G}$ の部分グラフとして定義される．この部分ブラフは** $\symbf{r}$ 次近傍エゴネットワーク** $\symscr{G}^{r}\_{v_i}$ と呼ばれる(Qiu *et al*., 2018a)． 形式的には， $r$ 次近傍エゴネットワーク $\symscr{G}^{r}\_{v_i}$ のノード集合 $\symscr{V}^{r}\_{v_i}$ とエッジ集合 $\symscr{E}^{r}\_{v_i}$ は次のように定義される：  

$$

\begin{aligned}
\symscr{V}^{r}_{v_i} &= \left\{v_j\in\symscr{V} \,\|\,\operatorname{dis}(v_i,v_j)\leq r\right\}\\
\symscr{E}^{r}_{v_i} &= \left\{(v_j,v_k)\in\symscr{E}\,\|\,v_j,v_k\in\symscr{V}^{r}_{v_i}\right\}
\end{aligned}
$$

  ここで， $\operatorname{dis}(v_i,v_j)$ はノード $v_i$ と $v_j$ の間の最短パスの長さを表す． さらに，各ノード $v_j \in \symscr{V}^{r}\_{v_i}/\left\\{v_i\right\\}$ には，関連付けられた二値のアクション状態 $s_j\in\left\\{0,1\right\\}$ が存在している． 例えば，Twitterの場合，ユーザ $v_j$ が特定のトピックの投稿をリツイートしたとき，アクション状態 $s_j=1$ となり，そうでないときは $s_j=0$ となる． すべてのノード $v_j \in \symscr{V}^{r}\_{v_i}/\left\\{v_i\right\\}$ のアクション状態は， $\symscr{S}^{r}\_{v_i} =\left\\{s_j\,\|\,v_j \in\symscr{V}^{r}\_{v_i}/\left\\{v_i\right\\}\right\\}$ としてまとめることができる．

社会的影響の予測が目指すのは， $\symscr{G}^{r}\_{v_i}$ と $\symscr{S}^{r}\_{v_i}$ が与えられた状況下で，ノード $v_i$ のアクション状態を予測することである． これは二値分類問題としてモデル化することになる．

ノード $v_i$ のアクション状態 $s_i$ を予測するためには，エゴネットワークにGNNを適用してノード $v_i$ の表現を学習し，その後，この表現を使った二値分類を実施する． GNNを構成するフィルタとして，Qiu *et al*.(2018a)ではGCNフィルタやGATフィルタが採用されている（GCNフィルタとGATフィルタの詳細については5.3.2節を参照せよ）． 以下の式(12.1)に示した特徴量は，本GNNモデルにおける，各ノードに与えられる初期値として利用される．

 $$
 
\tag{12.1}
\symbf{F}^{(0)}_j = [\symbf{x}_j,\,\symbf{e}_j,\,s_j,\,\operatorname{ind}_j],\quad v_j\in\symscr{V}^{r}_{v_i} $$
 

式(12.1)では， $\symbf{x}\_j$ は「ノード $v_j$ の事前学習された埋め込み」を示しており，例えばグラフ $\symscr{G}$ 全体に対してDeepWalkやLINEのような手法で学習した埋め込みを用いる（DeepWalkやLINEの詳細については4.2.1節を参照せよ）． なお，Qiu *et al*.(2018a)では，モデルの性能を向上させるため，これら $\symscr{V}^{r}\_{v_i}$ の埋め込みを正規化する手法であるインスタンス正規化(instance normalization)
[^2]
．

また，ベクトル $\symbf{e}\_j$ には，構造的特徴量（各種中心性など），内容的特徴量（ユーザ投稿物など），利用可能な場合は人口統計学的特徴量（ユーザ属性）といった，別のノード特徴量が含まれている． また， $s_j$ は $v_j$ のアクション状態を表しているが，未知であるため $0$ で初期化する． 最後の要素である $\operatorname{ind}\_j\in\left\\{0,\,1\right\\}$ は二値変数であり，ノード $v_j$ が「中心となるユーザ $v_i$ であるか（この中心となるユーザはエゴユーザ(ego user)と呼ばれる）」を示している． すなわち， $v_j=v_i$ のときのみ $\operatorname{ind}\_j=1$ ，それ以外の場合は $\operatorname{ind}\_j=0$ となる変数である．

#### 社会的関係性の表現学習

Facebookに代表されるソーシャルメディアの急速な発展に伴い，ソーシャルネットワークでユーザに提供されるサービスはますます増えている． 例えば，ユーザはFacebook上で様々な映画やスポーツ，書籍に対する好みを表現することができる． こうした様々な種類のSNSが利用可能になったことで，ユーザの行動パターンも多種多様になっている．

ユーザは，ある種類の行動については類似した嗜好を持つ一方，他の行動では全く異なる嗜好を持つことがある． 例えば，2人のユーザがいて彼らが「同じ種類の映画は好きでも，好きなスポーツの種類は全く異なる」ということは大いに有り得る． 異なる行動におけるユーザの嗜好の類似性を捉えるとき，各ユーザを表現するために複数のベクトルが利用される． 各ベクトルは特定の行動カテゴリに対応している(Wang *et al*., 2019a)．

具体的には，各ユーザについて，各行動カテゴリーに関する表現は条件付き表現として，そのユーザに関する全般的な特性（一般的な表現）を考慮した上で定義される． これらのユーザ表現を学習するために，GNNモデルが適応されている(Wang *et al*., 2019a)． まず，問題設定を詳しく説明し，その後，条件付き表現を学習するために開発されたGNNモデルを紹介する．

ソーシャルネットワークはグラフ $\symscr{G}=\left\\{\symscr{V}, \symscr{E}\right\\}$ としてモデル化することができる． ここで， $\symscr{V}=\left\\{v_1,\dots,\,v_N\right\\}$ はノード（ソーシャルユーザ）の集合を表し， $\symscr{E}$ はそれらをつなぐエッジ（ソーシャル関係）を示す． ユーザ間のこれらの関係は，グラフの隣接行列 $\symbf{A}$ によっても表現することができる． さらに，ユーザは映画や本，スポーツなど，様々なカテゴリに分類されたアイテムとも相互作用を持つ． 特に，カテゴリ $c$ （例えば本）のアイテム集合は $\symscr{I}\_c$ と表され，ユーザとこれらのアイテムとの相互作用は相互作用行列 $\symbf{R}^{c}$ によって説明される． ここで，ユーザ $v_i$ がカテゴリ $c$ に属する $j$ 番目のアイテムと相互作用を持った場合のみ $\symbf{R}^{c}\_{i,j}=1$ となり，そうでない場合は $0$ となる． 条件付き表現学習の目的は，各ユーザ $v_j$ の嗜好を反映するような一連の表現を学習することである． これらのデータ表現は**条件付き表現**と呼ばれる． 具体的には，各カテゴリ $c$ に対する条件付き表現は，ユーザ間の関係を示す構造情報の行列 $\symbf{A}$ と，そのカテゴリ $c$ に対するユーザの好みを示す行列 $\symbf{R}^{c}$ を捉えることができる． この表現学習は，5.3.2節で紹介されるMPNNフレームワークに基づいて設計されている． 今回フレームワークにおける（ $l$ 層目の）メッセージ関数 $M(\cdot)$ と更新関数 $U(\cdot)$ は次のように記述される． MPNNフレームワークのメッセージ関数 $M(\cdot)$ は，近傍ノード $v_j$ から中心ノード $v_i$ に渡るメッセージを生成する． 異なるカテゴリにおけるノード $v_i$ と $v_j$ の間の様々な類似性を捉えるために，これらのノード表現は以下のように異なるカテゴリにマッピングされる：  

$$
 \symbf{F}^{(l-1)}_{j\,\|\,c} = \symbf{F}^{(l-1)}_j\odot\symbf{b}^{(l-1)}_c $$


  ここで， $\symbf{b}^{(l-1)}\_c$ は，入力表現 $\symbf{F}^{(l-1)}\_j$ をカテゴリ $c$ の条件付き表現 $\symbf{F}^{(l-1)}\_{j\,\|\,c}$ に写像する，すべてのノードで共有される学習対象のバイナリマスク(binary mask)である． そして，ノード $v_j$ からノード $v_i$ へのメッセージは次のように生成される：  

$$
 \symbf{F}^{(l-1)}_{v_j\to v_i} = M(\symbf{F}^{(l-1)}_i,\,\symbf{F}^{(l-1)}_j) = \sum^{C}_{c=1}\alpha^{(l-1)}_{i,j\,\|\,c}\cdot\symbf{F}^{(l-1)}_{j\,\|\,c} $$


  上式において， $C$ はカテゴリの総数を表し， $\alpha^{(l-1)}\_{i,j\,\|\,c}$ は以下のように学習されたアテンションスコアである：  

$$

\begin{aligned}
    e^{(l-1)}_{i,j\,\|\,c} &= \symbf{h}^{(l-1)^{\top}}\operatorname{RELU}\left(\left[\symbf{F}^{(l-1)}_{i\,\|\,c},\,\symbf{F}^{(l-1)}_{j\,\|\,c}\right]\symbf{\Theta}^{(l-1)}_a\right)\\
    \alpha^{(l-1)}_{i,j\,\|\,c} &= \dfrac{\exp\left\{e^{(l-1)}_{i,j\,\|\,c}\right\}}{\sum^{C}_{c=1}\exp\left\{e^{(l-1)}_{i,j\,\|\,c}\right\}}
\end{aligned}
$$

  ここで， $\symbf{h}^{(l-1)}$ と $\symbf{\Theta}^{(l-1)}\_u$ は学習対象のパラメータである． アテンション機構は，ユーザ間の行動がより似ているほど，その行動がメッセージ生成に大きく貢献するように設計されている． メッセージを生成した後，ノード $v_i$ の表現は次のような更新関数で更新される：

  

$$

\begin{eqnarray}
\symbf{m}^{(l-1)}_i &=& \sum_{v_j\in\symscr{N}(v_i)}\symbf{F}^{(l-1)}_{v_j\to v_i}
\tag{12.2}\\
\symbf{F}^{(l)} &=& U(\symbf{F}^{(l-1)}_i,\,\symbf{m}^{(l-1)}_i) = \alpha\left(\left[\symbf{F}^{(l-1)}_i,\,\symbf{m}^{(l-1)}_i\right]\symbf{\Theta}^{(l-1)}_u\right)
\tag{12.3} \\ \nonumber
\end{eqnarray}
$$

  

ここで， $\Theta^{(l-1)}å$ は更新関数のパラメータであり， $\alpha\left(\cdot\right)$ は何らかの活性化関数を表している． 上記のMPNNフィルタリング操作を $L$ 層分重ねることで，最終的な表現 $\symbf{F}^{(L)}\_i$ が得られる． これらはその後，条件付き表現 $\symbf{F}^{(L)}\_{i\,\|\,c}$ に写像される． この最終的な条件付き表現 $\symbf{F}^{(L)}\_{i\,\|\,c}$ は，本フレームワークの学習目標となるユーザ・アイテム間のインタラクション情報 $\symbf{R}^{c}$ を復元するために利用される． したがって，学習された条件付き表現は，特定のカテゴリにおける社会構造情報とユーザ・アイテム間のインタラクション情報の両方を捉えることができる．

### 推薦システム

推薦システムは，ECサービス，動画・音楽ストリーミングサービス，ソーシャルメディアといった多くのオンラインサービスで活用されており，情報があふれる現代において，求める情報を絞り込む手助けをしてくれる． ユーザの過去の行動データを利用してその嗜好を予測する**協調フィルタリング（CF: Collaborative filtering）**(Goldberg *et al*., 1992; Resnick and Varian, 1997; Goldberg *et al*., 2001)は，推薦システムを開発するための最も重要な技術の一つである． 協調フィルタリング技術の重要な仮定は，「過去の行動が似ているユーザ同士は嗜好も似ている」ということである． 協調フィルタリングの手法は通常，ユーザとアイテムのベクトル表現にこのような情報（ユーザの過去の行動パターンや，それに基づくユーザ間やアイテム間の類似性）を符号化し，過去のインタラクションを再構成する(Koren *et al*., 2009; Wang *et al*., 2019h)． これらの表現を学習する際，通常，過去のインタラクションは明示的には利用されず，再構成のための真値(ground truth)としてのみ使用される．

ユーザとアイテム間のこれらの過去のインタラクションは，二部グラフ $\symscr{G} = \left\\{\symscr{U}\cup\symscr{V},\,\symscr{E}\right\\}$ を使ってモデル化することができる． 具体的には，ユーザ集合を $\symscr{U}=\left\\{u_1,\dots,u_{N_u}\right\\}$ ，アイテム集合を $\symscr{V}=\left\\{v_1,\dots,u_{N_v}\right\\}$ と表し，それら間のインタラクションを $\symscr{E}=\left\\{e_1,\dots,e_{N_e}\right\\}$ として表現する（ $e_i = (u_{(i)},v_{(i)}),\;u_{(i)}\in \symscr{U},\;v_{(i)}\in \symscr{V}$ ）． また，これらのインタラクションはインタラクション行列 $\symbf{M}\in\mathbb{R}^{N_u\times N_v}$ によって記述することも可能である． ここで， $\symbf{M}$ の $(i,j)$ 要素はユーザ $u_i$ とアイテム $v_j$ の間のインタラクション状態を示している． 具体的には， $\symbf{M}\_{i,j}$ はユーザ $u_i$ がアイテム $v_j$ に与えた評価値(rating value)を表していたり，一方でバイナリ評価とすることも可能である（バイナリ評価の場合， $\symbf{M}\_{i,j}=1$ はユーザ $u_i$ がアイテム $v_j$ との間にインタラクションがあったことを示す）
[^3]
．

二部グラフを用いると．GNNを採用することで，ユーザとアイテムの表現をモデル化するために，過去のインタラクションを明示的に利用することができる(Berg *et al*., 2017; Ying *et al*., 2018b; Wang *et al*., 2019h)． さらに，ユーザ間のつながりやアイテムの知識グラフなど，ユーザやアイテムに関する付加情報もグラフの形でモデル化することができる． これらの情報もまた，GNNモデルによる表現を学習するために取り入れられている(Wang *et al*., 2019b, c, g; Fan *et al*., 2019)． 次に，GNNモデルに基づく代表的な協調フィルタリング手法を紹介する．

#### 協調フィルタリング

典型的には，協調フィルタリングのアプローチはエンコーダ・デコーダモデルとみなすことができる． エンコーダは各ユーザやアイテムをベクトル表現に符号化し，デコーダはこれらの表現を利用して過去のインタラクションを再構成する． したがって，デコーダは通常，回帰タスク（評価値を再構成する場合）または二値分類タスク（インタラクションの存在を再構成する場合）としてモデル化される． そこで，主にGNNモデルに基づいて設計されたエンコーダ部分を紹介する． ユーザとアイテムの表現を更新するために，空間型のグラフフィルタリング操作が採用される． 具体的には，与えられたユーザについて，その表現は，隣接するアイテム，すなわち近傍ユーザとインタラクションを行うアイテムからの情報を利用して更新される． 同様に，与えられたアイテムについて，その表現は，隣接するユーザ，すなわち近傍アイテムとインタラクションを行うユーザからの情報を利用して更新される．

次に，あるユーザ $u_i$ の視点からグラフフィルタリングの過程を説明する（アイテムのグラフフィルタリングの過程も類似している）．（ $l$ 層目の）グラフフィルタリングは，以下のように，5.3.2節で紹介したMPNNフレームワークを使用して一般的に説明することができる：

  

$$

\begin{eqnarray}
\symbf{m}^{(l-1)}_i &=& \operatorname{AGGREGATE}\left(\left\{M\left(\symbf{u}^{(l-1)}_i,\,\symbf{v}^{(l-1)}_j,\,\symbf{e}_{(i,j)}\right)\,\|\,v_j\in\symscr{N}(u_i)\right\}\right)
\tag{12.4}\\
\symbf{u}^{(l)}_i &=& U\left(\symbf{u}^{(l-1)}_i,\symbf{m}^{(l-1)}_i\right)\nonumber
\end{eqnarray}
$$

  

ここで， $\symbf{u}^{(l-1)}\_i$ と $\symbf{v}^{(l-1)}\_j$ は $l$ 層目のユーザ $u_i$ とアイテム $v_j$ の入力表現をそれぞれ示しており， $\symbf{e}\_{(i,j)}$ はエッジ情報（利用可能であればユーザがアイテムに与えた評価値などの情報）を示している．  $\symscr{N}(u_i)$ はユーザ $u_i$ の近傍ノード，つまり，そのユーザとのインタラクションを持つアイテムを表す． そして， $\operatorname{AGGREGATE}(\cdot)$ ， $M(\cdot)$ ， $U(\cdot)$ はそれぞれ設計すべき集約関数，メッセージ関数，更新関数である．

Berg *et al*.(2017)では，いくつかの集約関数が提案されている（例：総和）． そしてメッセージ関数は，ユーザとアイテム間のインタラクションに伴う具体的な評価値の情報を取り入れるように設計されている：  

$$
 M\left(\symbf{u}^{(l-1)}_i,\,\symbf{v}^{(l-1)}_j\right)   = \dfrac{1}{\sqrt{\|\symscr{N}(u_i)\|\,\|\symscr{N}(v_j)\|}} \symbf{v}^{(l-1)}_j\symbf{\Theta}^{(l-1)}_{r(u_i,v_j)} $$


  上式において， $r(u_i,\,v_j)$ はユーザ $u_i$ がアイテム $v_j$ に付けた離散的な評価（例えば，1〜5）を表し， $\symbf{\Theta}^{(l-1)}\_{r(u_i,v_j)}$ はその評価値を持つ全てのインタラクションで共有されるパラメータである． また，更新関数は以下のように実装されるている：  

$$
 U(\symbf{u}^{(l-1)}_i,\,\symbf{m}^{(l-1)}_i) = \operatorname{ReLU}(\symbf{m}^{(l-1)}_i\symbf{\Theta}^{(l-1)}_{\text{up}}) $$


  ここで， $\symbf{\Theta}^{(l-1)}\_{\text{up}}$ は学習対象のパラメータである．

一方で，Wang *et al*.(2019h)では， $\operatorname{AGGREGATE}(\cdot)$ 関数として総和が採用され，メッセージ関数および更新関数を以下のように実装している：  

$$

\begin{aligned}
M(\symbf{u}^{(l-1)}_i,\symbf{v}^{(l-1)}_j) &= \dfrac{1}{\sqrt{\|\symscr{N}(u_i)\|\,\|\symscr{N}(v_j)\|}}\left(\symbf{v}^{(l-1)}_j\symbf{\Theta}^{(l-1)}_1 + (\symbf{u}^{(l-1)}_i\symbf{\Theta}^{(l-1)}_2\odot\symbf{v}^{(l-1)}_j)\right)\\
U(\symbf{u}^{(l-1)},\symbf{m}^{(l-1)}_i) &= \operatorname{LeakyReLU}\left(\symbf{u}^{(l-1)}_i\symbf{\Theta}^{(l-1)}_3 + \symbf{m}^{(l-1)}_i\right)
\end{aligned}
$$

  ここで， $\symbf{\Theta}^{(l-1)}\_1,\,\symbf{\Theta}^{(l-1)}\_2,\,\symbf{\Theta}^{(l-1)}\_3$ は学習対象のパラメータである．

#### アイテムに関する付加情報を用いた協調フィルタリング

アイテム間の関係を記述する知識グラフ(Knowledge graph)は，過去のインタラクションとは別の情報リソースとして利用される． アイテムの表現を学習しながら，知識グラフに符号化された情報を組み込むために，GNNモデルが採用されている(Wang *et al*., 2019c, b, g)．

アイテム集合 $\symscr{V}$ をエンティティとする知識グラフは， $\symscr{G}\_k = \left\\{\symscr{V},\symscr{E}\_k,\symscr{R}\right\\}$ として表現される． ここで， $\symscr{R}$ は知識グラフ内の関係集合(relational edge)を示しており，各関係エッジ $e\in\symscr{E}\_k$ は $e=\left\\{v_i,r,v_j\right\\}$ として表現できる（ $r\in \symscr{R}$ ）． アイテム $v_i$ に対して，知識グラフ内で関連付けられている他のアイテムは，情報を集約するための新たな情報源を提供することになる．

さまざまな関係の重要性を区別しながら情報を集約するために，アテンション機構が採用される． 具体的には，Wang *et al*.(2019g)では，関係 $(v_i,r,v_j)$ に対するアテンションスコア $\alpha_{i\,r\,j}$ は，知識グラフ埋め込み手法であるTransR (Lin *et al*., 2015)のアイデアに従って次のように計算される：  

$$

\begin{aligned}
\pi(v_i, r, v_j) = \left(\symbf{v}^{(0)}_j\symbf{\Theta}^{(l-1)}_r\right)^{\top}\tanh\left(\symbf{v}^{(0)}_i\symbf{\Theta}^{(l-1)}_r + \symbf{e}_r\right)\\
\alpha_{i\,r\,j} = \dfrac{\exp (\pi(v_i,r,v_j))}{\displaystyle\sum_{(r,v_j)\in\symscr{N}^{k}(v_i)}\exp(\pi(v_i, r, v_j))}
\end{aligned}
$$

  ここで， $\symbf{v}^{(0)}\_i,\,\symbf{e}\_r,\symbf{\Theta}^{(l-1)}\_r$ はTransR (Lin *et al*., 2015)から学習したエンティティ埋め込み，関係埋め込み，変換行列であり， $\symscr{N}^{k}(v_i)$ は知識グラフ $\symscr{G}\_k$ 内の $v_i$ の近傍を示す． アイテム $v_i$ の表現を更新するための（ $l$ 層目における）グラフフィルタリング処理は次のようになる：

 

$$

\begin{aligned}
&\symbf{m}^{(l-1)}_i = \sum_{(r,v_j)\in\symscr{N}^{k}(v_i)}\alpha_{i\,r\,j}\symbf{v}^{(l-1)}_j\notag\\
&\symbf{v}^{(l)}_i = U(\symbf{v}^{(l-1)}_i,\symbf{m}^{(l-1)}_i) = \operatorname{LeakyReLU}([\symbf{v}^{(l-1)}_i,\symbf{m}^{(l-1)}_i]\symbf{\Theta}^{(l-1)}_{\text{up}})
\end{aligned}
\tag{12.5}
$$

 

ここで， $U(\cdot)$ は更新関数であり， $\symbf{\Theta}^{(l-1)}\_{\text{up}}$ は学習対象のパラメータである． TransRから学習した埋め込み $\symbf{v}^{(0)}$ は，最初の層への入力として使用される． また，TransRから学習されたエンティティの埋め込み，関係の埋め込み，そして変換行列は，式(12.5)によって示される伝播過程においては固定のものとして扱われる． その結果，アテンションスコア $\alpha_{i\,r\,j}$ は，グラフフィルタの各層間で共有されることになる． さらに，ユーザとアイテム間のインタラクションは，"インタラクション"という一つの関係として知識グラフに組み込まれる(Wang *et al*., 2019g)． 具体的には，各 $\symbf{e}\_i=(u_{(i)},v_{(i)})\in\symscr{E}$ は，関係エッジ $(u_{(i)},r,v_{(i)})$ に変換され， $r=\text{インタラクション}$ とする． こうすることによって，ユーザ表現とアイテム表現の両方は，式(12.5)を通して更新することができる．

一方で，Wang *et al*.(2019c, b)では．アテンションスコアが各ユーザに対してパーソナライズされるように設計されている． 特に，あるエンティティ $v_j$ が別のエンティティ $v_i$ に与える影響や関連性を考慮する際，アイテムの推薦対象となるユーザも考慮に入れるべきである． 例えば，ユーザに映画を推薦する場合，一部のユーザは特定の監督の映画を好むかもしれないし，他のユーザは特定の俳優が出演する映画を好むかもしれない． したがって，ユーザ $u_k$ にアイテムを推薦することに特化したアイテムの埋め込みを学習する際，集約のためのアテンションスコアは次のようにモデル化することができる：  

$$
 \pi(v_i,r,v_j\,\|\,u_k) = \symbf{u}^{T}_k\symbf{e}_r $$


  ここで， $\symbf{u}\_k$ と $\symbf{e}\_r$ は各ユーザの埋め込みと関係の埋め込みを指している． 具体的には，このプロセスは「ユーザごとに知識グラフを作成している」とも考えられる． 注目すべきは，Wang *et al*.(2019c, b)では，表現学習のために明示的に知識グラフが利用されている一方で，過去のインタラクションは再構成のための真値(ground-truth)としてのみ利用されている点である． そのため，推薦システムにおける行列因子分解のように(Koren *et al*., 2009)，ユーザ表現 $\symbf{u}\_k$ は単にランダムな値で初期化される．

#### ユーザに関する付加情報を用いた協調フィルタリング

 $\symscr{U}$ 内のユーザ同士の相互関係(interaction)を符号化した**ソーシャルネットワーク**は，（前述した）ユーザ・アイテム間のインタラクションをモデル化した二部グラフとは異なる情報リソースとして活用することができる． ソーシャルネットワークはグラフ $\symscr{G}\_s = \left\\{\symscr{U},\symscr{E}\right\\}$ としてモデル化することができる． ここで， $\symscr{U}$ はノード（ユーザ）の集合で， $\symscr{E}\_s$ はユーザ間の関係を記述するエッジ集合である．

Fan *et al*.(2019)では，これら両方の情報を利用してユーザとアイテムの表現を学習するGNNモデルが採用されている． 具体的には，アイテムの表現は，二部グラフ $\symscr{G}$ 内の近傍ノード（つまり，アイテムとインタラクションを持つユーザ）に由来する情報を集約することで更新される． これは，前述した協調フィルタリングを用いたGNNモデルと同様のものである． 一方でユーザについては， $2$ つのリソース（つまり，ユーザ・アイテム間のインタラクションを表す二部グラフ $\symscr{G}$ とソーシャルネットワーク $\symscr{G}\_s$ ）に由来する情報を集約することで，ユーザ表現を生成する． その具体的な表現は以下の通りである：  

$$
 \symbf{u}^{(l)}_i = \left[\symbf{u}^{(l)}_{i,\symscr{I}},\symbf{u}^{(l)}_{i,\symscr{S}}\right]\symbf{\Theta}^{(l-1)}_c $$


  ここで， $\symbf{u}^{(l)}\_{i,\symscr{I}}$ は，二部グラフ $\symscr{G}$ 内の近傍アイテムからの情報を集約することで学習されるユーザ $u_i$ の表現を示しており， $\symbf{u}^{(l)}\_{i,\symscr{S}}$ は，ソーシャルネットワーク $\symscr{G}\_k$ 内の近傍ユーザからの情報を集約することで学習されるユーザ $u_i$ の表現を示している．  $\symbf{\Theta}^{(l-1)}\_c$ は学習対象のパラメータである． 具体的には、 $\symbf{u}^{(l)}\_{i,\symscr{I}}$ はパラメータ $\symbf{\Theta}^{(l-1)}\_{\symscr{S}}$ を用いて次のように更新される：  

$$
 \symbf{u}^{(l)}_{i,\symscr{S}} = \sigma\left(\sum_{u_j\in \symscr{N}^{s}(u_i)}\symbf{u}^{(l-1)}_j\symbf{\Theta}^{(l-1)}_{\symscr{S}}\right) $$


  ここで， $\symscr{N}^{s}(u_i)$ はソーシャルネットワーク $\symscr{G}\_s$ 内のユーザ $u_i$ の近傍ユーザの集合である． 一方で， $\symbf{u}^{(l)}\_{i,\symscr{I}}$ はパラメータ $\symbf{\Theta}^{(l-1)}\_{\symscr{I}}$ を用いて生成される：

 $$
 \symbf{u}^{(l)}_{i,\symscr{I}} = \sigma\left(\sum_{v_j\in\symscr{N}(u_i)}\left[\symbf{v}^{(l-1)}_j,\symbf{e}_{r(i,j)}\right]\symbf{\Theta}^{(l-1)}_{\symscr{I}}\right)
\tag{12.6} $$
 

ここで， $\symscr{N}(u_i)$ はユーザ $u_i$ とインタラクションを持ったアイテムの集合であり， $\symbf{e}\_{r(i,j)}$ は評価値の情報である． Fan *et al*.(2019)では，評価値は離散的なスコアであり，評価情報 $\symbf{e}\_{r(i,j)}$ は学習対象の埋め込みとしてモデル化される．


[メインページ](../../index.markdown)

[章目次](./chap12.md)

[前の節へ](./subsection_01.md) [次の節へ](./subsection_03.md)

[^1]: 訳注：局所的な社会的影響とは，**近い**関係やつながりを持つ他のユーザから受ける影響を指す\[どの程度，つまり何次近傍までを近いとみなすかは調査ごとに異なる\]．例えば，ソーシャルメディアの文脈では，あるユーザAが近い関係にある友人(他ユーザ)からのレコメンドに基づいて新しい映画を見ることを決定した場合，その友人がユーザAに対して「局所的な社会的影響を持っていた」と言える．
[^2]: 訳注：インスタンス正規化とは，ネットワークの各層の出力をサンプル（インスタンス）ごとに正規化する手法である．具体的には，各サンプルの特徴量の値の平均と標準偏差を計算し，それを用いて正規化を行う．これにより，モデルの学習が安定し，結果として学習速度が向上し，性能が改善することが期待される．インスタンス正規化はバッチ正規化(3.6節参照)から派生した手法である．一方，Qiu *et al*.(2018a)では， $v_i$ の表現ベクトルの各次元を， $\symscr{V}^{r}\_{v_i}$ の表現ベクトルの次元ごとの平均・標準偏差を用いて正規化する手法を採用している．Qiuらはこの手法をインスタンス正規化と呼んでいるが，インスタンス( $v_i$ )以外のノードを使って正規化していることから，厳密には一般的なインスタンス正規化とは少し異なるアプローチを用いていることに留意されたい．
[^3]: 訳注：評価値の例としては，映画や商品への星評価（例えば，1〜5の範囲を取る）がある．また，「いいね」や「興味がある」のようなバイナリ評価（0または1を取る）も存在する．ユーザがアイテムに与える評価は，これらのようにさまざまな形を取る可能性がある．

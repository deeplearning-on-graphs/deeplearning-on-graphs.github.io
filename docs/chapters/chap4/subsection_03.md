[メインページ](../../index.markdown)

[章目次](./chap4.md)
## 4.3. 複合グラフ上のグラフエンベディング

これまでの節では，単純なグラフに対するグラフエンベディングアルゴリズムについて述べてきた．しかし，実世界のグラフは2.6節で示したようにもっと複雑なパターンを持っており，これらのパターンに対応した複雑なグラフが多種類存在している．
そこで本節では，このような複雑なグラフに対する埋め込み手法を紹介する．

### Heterogeneousグラフの埋め込み

Heterogeneousグラフでは，異なる種類のノードが存在している． Chang et
al.(2015)では，Heterogeneousグラフ上の異なる種類のノードを共通の埋め込み空間に投影する枠組みであるHeterogeneous
Network Embedding(HNE)が提唱されている．
この目的を達成するために，（ノードの）種類ごとに異なるマッピング関数を取り入れている．ここでノードは，（画像やテキストなどの）異なる形式・次元を有するような"ノードの特徴"に関連づけられると仮定する．
だから，ノードの種類ごとに異なる深層学習モデルを採用し，対応する特徴を共通の埋め込み空間にマッピングすることになる．例えば，関連する特徴が画像の形式である場合，マッピング関数としてはCNNが採用される．そしてHNEは，それらノード間の接続関係を保存することを目的としている．よってHNEにおける情報抽出は，再構成すべき情報としてエッジを持つノード組を抽出するが，これは当然ながら隣接行列 $\symbf{A}$ で表すことができる．
したがって情報の再構成はノードの埋め込みから隣接行列 $\symbf{A}$ を復元することである．具体的には，ノード $v\_i,v\_j$ とマッピング関数で学習したそれらの埋め込み $\symbf{u}_i,\symbf{u}_j$ の組が与えられたとき，再構成された隣接行列の要素 $\tilde{\symbf{A}}\_{i,j}=1$ の確率は以下のように計算される．
 

$$ p(\tilde{\symbf{A}}\_{i,j} =1) = \sigma(\symbf{u}\_i^{\top}\symbf{u}_j). $$

 
 $\sigma$ はシグモイド関数である．この式に対応する形で以下も計算できる．
 

$$ p(\tilde{\symbf{A}}\_{i,j} =0) = 1-\sigma(\symbf{u}\_i^{\top}\symbf{u}_j). $$

 
再構成された隣接行列 $\tilde{\symbf{A}}$ が下の隣接行列 $\symbf{A}$ に近くなるように，確率を最大化することが目標であることから，目的関数は交差エントロピーによって以下のようにモデル化される．

 

$$ -\sum^{N}\_{i,j=1}\left\(\symbf{A}\_{i,j}\log p(\tilde{\symbf{A}}=1) + (1-\symbf{A}\_{i,j})\log p(\tilde{\symbf{A}}\_{i,j} = 0)\right\). $$

 

この式(4.10)の目的関数を最小化することでマッピング関数を学習することができ，その結果として埋め込みが得られる．

Heterogeneousグラフでは，異なる種類のノードとエッジが異なる意味を持つことから，Heterogeneousネットワークの埋め込みではノード間の構造的な相互関係だけでなく，意味的な相互関係も気にする必要がある．metapath2vec(Dong
et al.,
2017)は，そのようなノード間の相互関係を両方捉えるために提案されたものである．
そこで次に，metapath2vecのアルゴリズムについて，その情報抽出や再構成，目的関数を絡めて詳しく説明していく．
なお，metapath2vecのマッピング関数はDeepWalkのマッピング関数と同じであることに注意．

### メタパスに基づく情報抽出 {#メタパスに基づく情報抽出 .unnumbered}

構造的相互関係と意味的相互関係の両方を捉えるために，"メタパス(meta-path)に基づくランダムウォーク"を導入することによって共起情報の抽出を行う．具体的には，ランダムウォークにおける移動先の決定を制約するためにメタパスが採用されている．そこでまずメタパスの概念を紹介してから，メタパスに基づくランダムウォークの設計方法についての説明に移ることにする．

<div class="definition">
 
<strong>定義 4.5 メタパス・スキーマ(meta-path
schema)</strong>

定義2.35で定めたようなHeterogeneousグラフ $\symbfscr{G}$ が与えられているとする．メタパス・スキーマ $\psi$ とは， $A\_1\xrightarrow{R\_1}A\_2\xrightarrow{R\_2}\cdots\xrightarrow{R\_l}A_{l+1}$ と表されるような $\symbfscr{G}$ のメタテンプレート(meta-template)を指している．ここで， $A\_i\in \symscr{T}_n$ や $R\_i\in\symscr{T}_e$ はノードやエッジの特定の種類を表す．メタパス・スキーマは，ノードの種類 $A\_1$ からノード種類 $A_{l+1}$ までのノード間の"合成関係(composite
relation)"を定義しており，その関係は $R = R\_1\circ R\_2\circ \cdots \circ R_{l-1}\circ R\_l$ と表せる．メタパス・スキーマ $\psi$ の実例(instance)はメタパスであり，パス中の各ノードとエッジはそのメタパス・スキーマ中の対応する種類に従っている．

</div>


ランダムウォークの手引き用にメタパス・スキーマを使うことができる．
与えられたメタパス・スキーマ $\psi$ からランダムに生成される実例(instance)が，"メタパスに基づくランダムウォーク"である．

<div class="definition">
 
<strong>定義 4.6</strong>

メタパス・スキーマ $\psi\colon A\_1\xrightarrow{R\_1}A\_2\xrightarrow{R\_2}\cdots\xrightarrow{R\_l}A_{l+1}$ が与えられているとする． $\psi$ によって手引きされるランダムウォークの遷移確率は以下のように計算される．
 $$ p(v^{(t+1)}|v^{(t)},\psi) = 
    \begin{cases}
     \dfrac{1}{|\symscr{N}^{R\_t}\_{t+1}(v^{(t)})|},& v^{(t+1)}\in\symscr{N}^{R\_t}\_{t+1}(v^{(t)})\\
     \qquad\; 0,& \quad\;\text{otherwise}.
    \end{cases} $$ 
 $v^{(t)}$ は種類 $A\_t$ に属するノードであり，メタパス・スキーマにおける $A\_t$ の位置に対応している． $\symscr{N}^{R\_t}\_{t+1}(v^{(t)})$ は，エッジの種類 $R\_t$ を通して $v^{(t)}$ とつながっている，ノードの種類 $A_{t+1}$ を持つ $v^{(t)}$ 近傍の集合を示している．
形式的には，この近傍は以下のように定義することができる．
 

$$ \symscr{N}^{R\_t}\_{t+1}(v^{(t)}) = \left\\{v\_j | v\_j \in \symscr{N}(v^{(t)})\;\text{かつ}\;\phi\_n(v\_j) = A_{t+1}\;\text{かつ}\;\phi\_e(v^{(t)},v\_j) = R\_t\right\\}. $$

 
ここで， $\phi\_n(v\_j)$ はノード $v\_j$ の種類を取り出す関数で， $\phi\_e(v^{(t)},v\_j)$ はエッジ $(v^{(t)}, v\_j)$ の種類を取り出す関数である（いずれの関数もDefinition
2.35で導入している）． 
</div>

こうして，4.2.1節と同様の方法で共起するノード組を抽出することができる様々なメタパス・スキーマの手引きの下で，ランダムウォークを生成することができる．またこれまでと同様，ランダムウォークから $(v_{\text{con}}, v_{\text{cen}})$ の形で抽出されたノード組の集合を $\symscr{I}$ と表すことになる．

### 情報の再構成 {#情報の再構成 .unnumbered}

情報の再構成について，Dong et al.(2017)では二種類提案されている．
一つ目は，4.2.1節のDeepWalkの場合（式(4.2)）と同じである．
もう一方の再構成は，式(4.2)のような全ノードに対する単項分布(single
distribution)ではなく，ノードの種類ごとの多項分布(multinomial
distribution)を定義する方法である．
この場合，種類が $nt$ のノード $v\_j$ について， $v\_i$ が与えられたときに $v\_j$ を観測する確率は次のように計算される．
 

$$ p(v\_j|v\_i) = \dfrac{\displaystyle\exp (f_{\text{con}}(v\_j)^{\top}f_{\text{cen}}(v\_i))}{\displaystyle\sum_{v\in \symbfscr{V}_{nt}}\exp(f_{\text{con}}(v)^{\top}f_{\text{cen}}(v\_i))} $$

 
 $\symbfscr{V}_{nt}$ は，種類 $nt\in\symscr{T}_n$ を持つすべてのノードから成る集合である．
以上の二つの再構成のどちらかを採用すれば，4.2.1節のDeepWalkと同じ方法で目的関数を構成できる．

### 二部グラフの埋め込み

Definition
2.36で定義されているように，二部グラフでは，二つの互いに素となるノード集合である $\symbfscr{V}\_1$ と $\symbfscr{V}\_2$ が存在しており，この二つのそれぞれの集合内にはエッジが存在しない．便宜上の理由から，この互いに素である二つの集合を $\symscr{U}$ および $\symscr{V}$ と表すことにする．
Gao et
al.(2018b)では，二つの集合間の関係や各集合内の関係を捉えるために，二部グラフの埋め込みフレームワークBiNE（Bipartite
Network Embedding）が提案されている．
ここでは以下の二種類の情報を抽出している．

(1) 二つの集合内のノード同士を接続するエッジ集合 $\symbfscr{E}$ ．

(2) 各集合内のノード同士の共起情報．

二つの集合に含まれるノードをノード埋め込みに対応させるために，DeepWalkと同じマッピング関数が採用されている．
ここでは，ノード $u\_i\in\symscr{U}$ と $v\_i\in\symscr{V}$ に対する埋め込みをそれぞれ $\symbf{u}_i$ と $\symbf{v}_i$ と表記することにする．次に，BiNEの情報抽出，情報再構成，目的関数について説明していく．

### 情報抽出 {#情報抽出-2 .unnumbered}

二部グラフからは二種類の情報が抽出される．一つは， $\symbfscr{E}$ と表記される，二つノード集合内のノード間のエッジである．各エッジ $e\in\symbfscr{E}$ は， $u_{(e)}\in\symscr{U}$ および $v_{(e)}\in\symscr{V}$ を用いて $(u_{(e)},v_{(e)})$ と表される．もう一方の情報は，各ノード集合内におけるノード間の共起情報である．各ノード集合内の共起情報を抽出するためには， $\symscr{U}$ と $\symscr{V}$ をノード集合とする二つのホモジニアスグラフを二部グラフから生成する．
具体的に，元のグラフにおいて二つのノードが二次近傍で隣接していれば，これらのノードは生成したグラフで（直接）接続されることになる．ノード集合 $\symscr{V}$ および $\symscr{U}$ に対して生成されるグラフをそれぞれ $\symbfscr{G}_{\symscr{U}}$ および $\symbfscr{G}_{\symscr{V}}$ とする．
こうすることで，DeepWalkと同じように，二つのグラフから共起情報を抽出することができる．抽出された共起情報をそれぞれ $\symscr{I}\_{\symscr{U}}$ および $\symscr{I}\_{\symscr{V}}$ と表記する．したがって，再構成される情報は，エッジの集合 $\symbfscr{E}$ ， $\symscr{U}$ と $\symscr{V}$ の共起情報である．

### 情報再構成および目的関数 {#情報再構成および目的関数 .unnumbered}

 $\symscr{U}$ と $\symscr{V}$ の共起情報を埋め込みから復元する情報再構成は，DeepWalkのものと同じである． $\symscr{U}$ と $\symscr{V}$ を再構成する二つの目的関数をそれぞれ $\symscr{L}\_{\symscr{U}}$ と $\symscr{L}\_{\symscr{V}}$ と表記する．一方でエッジ集合 $\symbfscr{E}$ を復元するためには，埋め込み情報に基づいてエッジを観測する確率をモデル化することになる．
具体的には， $u\_i\in\symscr{U}$ や $v\_j\in\symscr{V}$ としたノード組 $(u\_i,v\_j)$ が与えられたとき，元の二部グラフにおいて二つのノード間にエッジが存在する確率を
 

$$ p(u\_i, u\_j) = \sigma(\symbf{u}\_i^{\top}\symbf{v}_j), $$

 
と定義する（ $\sigma$ はシグモイド関数）．
このとき， $\symbfscr{E}$ におけるエッジのノード組に対する確率が最大となるような埋め込みを学習することが目標である．
したがって，目的関数は
 

$$ \symscr{L}\_{\symbfscr{E}} = -\sum_{(u\_i, v\_j)\in\;\symbfscr{E}} \log p(u\_i, v\_j), $$

 
と定義される．

以上より，BiNEにおける最終的な目的関数は以下のようになる．
 

$$ \symscr{L} = \symscr{L}\_{\symbfscr{E}} + \eta\_1\symscr{L}\_{\symscr{U}} + \eta\_2\symscr{L}\_{\symscr{V}}. $$

 
ここで， $\eta\_1$ と $\eta\_2$ は異なる種類の情報に対する寄与を調整するためのハイパーパラメータである．

### 多次元グラフの埋め込み

多次元グラフでは，全ての次元が同じノード集合を共有しつつ，それぞれ独自のグラフ構造を持っている．そこで，各ノードに対して以下の学習を目指すこととする(Ma
et al., 2018d)．

(1) 全ての次元の情報を取り込んだ全体的ノード表現．

(2) 対応する次元により一層焦点を当てた，各次元に対応する次元別ノード表現．

全体的ノード表現は，全次元のノード情報を必要とするノード分類などのような全体的なタスクを実行するために利用することができる．一方で次元別ノード表現は，ある特定の次元におけるリンク予測など，次元に特化したタスクを実行するために利用できる．
直感的には，各ノードについて，全体的な表現と次元別の表現は独立でない．
したがって，その依存性をモデル化することが重要である．この目的を達成するためには，各次元 $d$ について，与えられたノード $v\_i$ の次元別表現 $\symbf{u}\_{d,i}$ を

 

$$ \symbf{u}\_{d,i} = \symbf{u}\_i + \symbf{r}\_{d,i}, $$

 

としてモデル化する． $\symbf{u}\_i$ は全体的表現で， $\symbf{r}\_{d,i}$ は依存性を考えない次元 $d$ の情報のみを取り込んだ表現である．
これらの表現を学習するために，異なる次元における共起関係の再構成を目指す．
具体的には，異なる次元から抽出された共起関係を再構成することで， $\symbf{u}_i$ と $\symbf{r}\_{d,i}$ のマッピング関数を最適化する．次に，多次元グラフの埋め込みに関するマッピング関数，情報抽出，情報再構成，目的関数について紹介していく．

### マッピング関数 {#マッピング関数-1 .unnumbered}

全体的表現に関するマッピング関数を $f()$ で表し，特定の次元 $d$ に関するマッピング関数を $f\_d()$ と表すことにする．なお，マッピング関数は全てのDeepWalkのものと同様である．これらは，以下のようにルックアップテーブルとして実装される．
 $$ \begin{aligned}
    \symbf{u}_i &= f(v\_i) = e^{\top}\_i\symbf{W},\\
    \symbf{r}\_{d,i} &=f\_d(v\_i) = e^{\top}\_i\symbf{W}_d,\;\; (d=1,\dots,D).\end{aligned} $$ 
 $D$ は多次元グラフにおける次元の個数である．

### 情報抽出 {#情報抽出-3 .unnumbered}

4.2.1節で導入した共起関係の情報抽出を使って，各次元 $d$ の共起関係を $\symscr{I}_d$ として抽出する．
そして全次元の共起情報は，以下のように各次元の共起情報の和集合となる．
 

$$ \symscr{I} = \bigcup^{D}\_{d=1}\symscr{I}_d. $$

 

### 情報再構成および目的関数 {#情報再構成および目的関数-1 .unnumbered}

共起 $\symscr{I}$ の確率をうまく再構成できるようなマッピング関数を学習することを目指す．情報再構成はDeepWalkのものと類似している．唯一の違いは，異なる次元から抽出された関係に対して再構成を適用することにある．
これに対応して，目的関数は以下のように記述できる．

 

$$ \min_{\symbf{W},\symbf{W}_1,\dots,\symbf{W}_D} - \sum^{D}\_{d=1}\sum_{(v_{\text{con}}, v_{\text{cen}})\in\;\symscr{I}_d}\;\#(v_{\text{con}}, v_{\text{cen}})\cdot\log p(v_{\text{con}}|v_{\text{cen}}). $$

 

 $\symbf{W},\symbf{W}_1,\dots,\symbf{W}_D$ は学習対象のマッピング関数のパラメータである．なお，Ma
et
al.(2018d)では，あるノードに対して，中心表現と文脈表現の両方に同じ表現が使われる．

### 符号付きグラフの埋め込み

符号付きグラフでは，Definition
2.38で導入したように，正のエッジと負のエッジの両方がノード間に存在している．
"構造バランス理論(Structural balance
theory)"は，符号付きグラフにとって最も重要な社会理論の一つである．
構造バランス理論に基づく符号付きグラフの埋め込みアルゴリズムSiNE(Signed
Network Embedding)が提案されている(Wang et al., 2017b)
バランス理論が示唆するように(Cygan et al.,
2012)，ノードは「敵」（または負のエッジを持つノード）よりも「友」（または正のエッジを持つノード）に近いはずである．
例えば図4.4では， $v\_j$ と $v\_k$ はそれぞれ $v\_i$ の友と敵とみなすことができる．
SiNEは，埋め込みドメインにおいて敵よりも友に近い位置にマッピングすること，すなわち $v\_j$ を $v\_k$ よりも $v\_i$ に近い位置にマッピングすることを目指している．
したがって，SiNEが保存すべき情報は，敵と友の間の相対的な関係である．
なお，SiNEにおけるマッピング関数は，DeepWalkと同じものである．次に，情報抽出について説明し，その後に再構成を導入していく．

### 情報抽出 {#情報抽出-4 .unnumbered}

保存すべき情報は，図4.4に示すように，ノード $v\_i$ と $v\_j$ を正のエッジで，ノード $v\_i$ と $v\_k$ を負のエッジで接続した三つ組 $(v\_i,v\_j,v\_k)$ で表現することができる．
この三つ組を符号付きグラフの集合として $\symscr{I}_1$ と表すと，形式的には以下のように定義できる．
 

$$ \symscr{I}_1 = \left\\{\left\(v\_i,v\_j,v\_k\right\) | \symbf{A}\_{i,j}=1,\;\;\symbf{A}\_{i,k}=-1,\;\;v\_i,v\_j,v\_k\in \symbfscr{V}\right\\}. $$

 
ここで， $\symbf{A}$ はDefinition
2.38で定義された符号付きグラフの隣接行列である．
三つ組 $(v\_i,v\_j,v\_k)$ では，バランス理論により，ノード $v\_j$ はノード $v\_k$ よりも $v\_i$ に類似しているはずである．
あるノード $v$ についての" $2$ 次サブグラフ"を，ノード $v$ とその $v$ から $2$ 次近傍以内のノード，そしてそれらのノード間のすべてのエッジによって形成されるサブグラフとして定義する．
実際，その $2$ 次サブグラフが正または負のエッジしか持たないようなノード $v$ に対しては，抽出情報 $\symscr{I}$ には何の情報も含まれない．この場合，図4.5に示すように， $v$ を含む全ての三つ組は同じ符号のエッジを含むことになる．ゆえに，これらのノード表現を学習するために，これらのノードに対して保存すべき情報を指定する必要がでてくる．

負のエッジを形成するコストは，正のエッジを形成するコストより高いことは明らかである(Tang
et al., 2014b)．
例えばSNSでは，多くのノードがその $2$ 次サブグラフで正のエッジしか持たず，それに対して $2$ 次サブグラフで負のエッジだけを持つようなノードは非常に少ないことがわかる．
したがって，ここでは $2$ 次サブグラフが正のエッジのみを持つノードの処理だけを考えるが，もう一方の種類のノードの処理に対しても同様の戦略が適用できる．
これらのノードの情報を効果的に取り込むためには，仮想ノード $v\_0$ を導/入し，仮想ノード $v\_0$ とこれらの各ノードとの間に負のエッジを作成する．
このようにして，図4.5aに示すような三つ組 $(v\_i,v\_j,v\_k)$ は，図4.6に示すように二個の三つ組 $(v\_i,v\_j,v\_0)$ と $(v\_i,v\_k,v\_0)$ に分割することができる．ここで，仮想ノード $v\_0$ が関与する全てのエッジを $\symscr{I}_0$ とすると，抽出情報は $\symscr{I} = \symscr{I}_1\cup\symscr{I}_0$ と表せる．

### 情報の再構成 {#情報の再構成-1 .unnumbered}

与えられた三つ組ノードの情報を再構成するために，三つ組の相対的な関係をノード埋め込みに基づいて推論することを目指す．
ある三つ組ノード $(v\_i,v\_j,v\_k)$ に対して， $v\_i,\,v\_j,\,v\_k$ 間の相対関係は，それらの埋め込みを使って，以下のように数学的に再構成することができる．

 

$$ s(f(v\_i), f(v\_j)) - \Big(s(f(v\_i), f(c\_k)) + \delta \Big) $$

 

 $f()$ は式(4.1)と同じマッピング関数である．関数 $s(\cdot,\cdot)$ は与えられた $2$ つのノード表現の類似性を測定するもので，順伝播型ニューラルネットワークを用いてモデル化される[^11]．
式(4.13)が正の値であれば， $v\_i$ は $v\_k$ よりも $v\_j$ に類似していること，すなわち $s(f(v\_i),f(v\_j)) > s(f(v\_i), f(v\_k))$ となることが示唆される．より具体的には，式(4.13)の値が $0$ より大きいとき， $s(f(v\_i), f(v\_j)) > s(f(v\_i), f(v\_k)) + \delta$ となる．
パラメータ $\delta$ は， $2$ つの類似度の差を制御するための閾値である．式(4.13)の値を $0$ より大きくするためには， $s(f(v\_i), f(v\_j)) - s(f(v\_i), f(v\_k))$ は少なくとも $\delta$ より大きくなければならない．ゆえに， $\delta$ が大きくなると，式(4.13)の値を正に維持するためには， $v\_i$ と $v\_j$ の類似度が $v\_i$ と $v\_k$ の類似度よりもはるかに大きくなることが必要である．
そして， $\symscr{I}$ が含む任意の三つ組ノード $(v\_i, v\_j, v\_k)$ について，式(4.13)の値は相対的な情報を保存できるように $0$ より大きくすることが期待される．このことはつまり，正のエッジで接続された $v\_i$ と $v\_j$ は負のエッジで接続された $v\_i$ と $v\_k$ よりも類似度が高いことになる．

### 目的関数 {#目的関数 .unnumbered}

 $\symscr{I}$ の情報をノード表現によって確実に保存するためには， $\symscr{I}$ が含む全ての三つ組ノードに対して式(4.13)の値が $0$ より大きくなるようにマッピング関数を最適化する必要がある．
このことから，目的関数は以下のように定義できる．  $$ \begin{aligned}
\min_{\symbf{W},\symbf{\Theta}}\dfrac{1}{|\symscr{I}_0| + |\symscr{I}_1|}[&
\sum_{(v\_i,v\_j,v\_k)\in\;\symscr{I}_1}\max\Big(0, s(f(v\_i), f(v\_k)) + \delta - s(f(v\_i), f(v\_j))\Big)\\
    +&\sum_{(v\_i,v\_j,v\_0)\in\;\symscr{I}_0}\max\Big(0, s(f(v\_i), f(v\_0)) + \delta\_0 - s(f(v\_i), f(v\_j))\Big)\\
    +&\;\alpha(R(\symbf{\Theta}) + \|\symbf{W}\|^2_F)].\end{aligned} $$ 
ここで， $\symbf{W}$ はマッピング関数のパラメータ， $\symbf{\Theta}$ は $s(\cdot,\cdot)$ のパラメータを表している．また $R(\Theta)$ はパラメータ $\symbf{\Theta}$ に対する正則化である．
なお， $\symscr{I}$ と $\symscr{I}_0$ には異なるパラメータ $\delta$ と $\delta\_0$ を用いており， $2$ つの情報元からの三つ組を柔軟に区別している．

### ハイパーグラフの埋め込み

2.6.5節で説明したようにハイパーグラフでは，ハイパーエッジはノード集合間の関係を表している．
Tu et al.(2018)では，Deep Hyper Network
Embedding(DHNE)という，ハイパーエッジに符号化された関係を使ってハイパーグラフ上のノード表現を学習する手法が提案されている．
具体的には，埋め込みによって再構成されたハイパーエッジからは， $2$ 種類の情報を抽出することになる．
一つはハイパーエッジによって直接記述される近傍性で，もう一方はハイパーエッジ内のノードの共起性である．次にDHNEについて，その情報抽出や，マッピング関数，情報再構成，目的関数について説明していく．

### 情報抽出 {#情報抽出-5 .unnumbered}

ハイパーグラフからは $2$ 種類の情報が抽出される．一つはハイパーエッジである． $\symbfscr{E}$ と表記されるハイパーエッジの集合は，ノード間の関係を直接的に記述している．もう一つは，ハイパーエッジの共起情報である．ノード $v\_i$ と $v\_j$ のノード組に対して，それらがハイパーエッジで共起する頻度は，それらの関係がどの程度強いかを示している．
任意のノード組に対するハイパーエッジでの共起は，以下のように接続行列 $\symbf{H}$ から抽出することができる．
 

$$ \symbf{A} = \symbf{H}\symbf{H}^{\top} - \symbf{D}\_v. $$

 
ここで， $\symbf{H}$ は接続行列で， $\symbf{D}_v$ はDefinition
2.39で定義した対角ノード次数行列である． $i,\,j$ 要素 $\symbf{A}\_{i,j}$ は，ノード $v\_i$ と $v\_j$ がハイパーエッジで共起する階数を示す．するとノード $v\_i$ について， $\symbf{A}$ の $i$ 行目にはグラフ内の全ノードとの共起情報（またはノード $v\_i$ の大域的な情報）が記述されていることになる．
以上をまとめると，抽出情報はハイパーエッジの集合 $\symbfscr{E}$ と大域的な共起情報 $\symbf{A}$ を含んでいる．

### マッピング関数 {#マッピング関数-2 .unnumbered}

マッピング関数は，大域的な共起情報を入力とするような多層順伝播型ネットワークでモデル化される．
具体的には，ノード $v\_i$ に対して，以下のように処理を記述することができる．
 

$$ \symbf{u}_i = f(\symbf{A}_i;\,\symbf{\Theta}) $$

 
 $f$ は $\symbf{\Theta}$ をパラメータとして持った順伝播型ネットワークを表している．

### 情報再構成および目的関数 {#情報再構成および目的関数-2 .unnumbered}

抽出された $2$ 種類の情報を復元するために， $2$ つの情報再構成が用意される．
まず，ハイパーエッジ集合 $\symbfscr{E}$ を復元する再構成を説明し，そのあと共起情報 $\symbf{A}$ を復元する再構成を説明していく．
埋め込みからハイパーエッジの情報を復元するためには，任意のノード集合 $\left\\{v_{(1)},\dots,v_{(k)}\right\\}$ 内にハイパーエッジが存在する確率をモデル化し， $\symbfscr{E}$ 内のそれらのハイパーエッジの確率を最大化することを目指す．
便宜上，Tu et
al.(2018)では全てのハイパーエッジは $k$ 個のノード集合を持つと仮定している．
（この仮定の下で）与えられたノード集合 $\symscr{V}^{i} = \left\\{v^{i}\_{(1)},\dots,v^{i}\_{(k)}\right\\}$ にハイパーエッジが存在する確率は以下のように定義される．
 

$$ p(1|\symscr{V}^i) = \sigma\left\(g([\symbf{u}^{i}\_{(1)},\dots,\symbf{u}^{i}\_{(k)}])\right\). $$

 
 $g()$ はノード埋め込みの連結(concatenation)を一つスカラーに写像する順伝播型ネットワークで， $\sigma()$ はそのスカラーを確率に変換するシグモイド関数である．
ハイパーグラフ上の $\symscr{V}^i$ のノード間にハイパーエッジがあるかどうかを示す変数を $R^{i}$ とすると， $R^{i}=1$ はハイパーエッジがあることを示し， $R^{i}=0$ はハイパーエッジがないことを意味する．そして，目的関数は交差エントロピーに基づいて以下のようにモデル化される．
 

$$ \symscr{L}_1 = -\sum_{\symscr{V}^i\in\;\symbfscr{E}\cup\symbfscr{E}^{\prime}}R^i\log p(1|\symscr{V}^i) + (1-R^i)\log (1-p(1|\symscr{V}^i)). $$

 
ここで， $\symbfscr{E}^{\prime}$ はネガティブなサンプルとしてランダムに生成される"ネガティブハイパーエッジ"集合である．ネガティブなハイパーエッジ $\symscr{V}$ の各々は，ランダムにサンプリングされた $k$ 個のノードの集合から構成される．

一方，ノード $v\_i$ に対する大域的な共起情報 $\symbf{A}\_i$ を復元するためには，埋め込み $\symbf{u}_i$ を入力とする順伝播型ネットワークを
 

$$ \tilde{\symbf{A}}\_i = f_{\text{re}}(\symbf{u}_i;\,\symbf{\Theta}\_{\text{re}}) $$

 
として採用する． $f_{\text{re}}()$ は共起情報を再構成する順伝播型ネットワークで， $\symbf{\Theta}\_{\text{re}}$ をそのネットワークが持つパラメータとする．そして，目的関数は最小二乗法で
 

$$ \symscr{L}\_2 = \sum_{v\_i\in\;\symscr{V}}\|\symbf{A}_i - \tilde{\symbf{A}}_i\|^2_2 $$

 
と定義される． 以上より， $2$ つの目的関数を組み合わせることで，
 

$$ \symscr{L} = \symscr{L}_1 + \eta\symscr{L}_2. $$

 
としてネットワーク埋め込みフレームワーク全体の目的関数を形成できる． $\eta$ は $2$ つの目的関数のバランスを調整するためのハイパーパラメータである．

### ダイナミックグラフの埋め込み

ダイナミックグラフでは，2.6.6節で説明したように，各エッジにはその出現時刻を示す時点(timestamp)が付与されている．
よってノード表現を学習する際には，時間的な情報を捉えることが重要である．
Nguyen et
al.(2018)では，グラフの時間情報を捉えたランダムウォークを生成する"時間的ランダムウォーク(temporal
random walk)"が提案されている．
生成された時間的ランダムウォークは，再構成されるべき共起情報を抽出するために利用される．
マッピング関数，情報再構成，目的関数はDeepWalkと同じであるため，ここでは時間的ランダムウォークとそれに対応する情報抽出について主に説明していく．

### 情報抽出 {#情報抽出-6 .unnumbered}

時間的ランダムウォークはNguyen et
al.(2018)で導入され，時間的情報とグラフの構造的情報の両方を捉えることができる．有効となる時間的ランダムウォークは，時点が遡らないエッジで接続されたノード列(node
sequence)で構成される．
この時間的ランダムウォークを正式に導入するために，まず，与えられた時点 $t$ におけるノード $v\_i$ の時間的近傍で構成される集合を以下のように定義する．

<div class="definition">
 
<strong>定義 4.7 時間的近傍</strong>

ダイナミックグラフ $\symbfscr{G}$ のノード $v\_i\in\symscr{V}$ について，時点 $t$ におけるその時間的近傍は，時点 $t$ 以降に $v\_i$ と接続しているノードの集まりである．形式的には以下のように表すことができる．
 

$$ \symscr{N}\_{(t)}(v\_i) = \left\\{v\_j|(v\_i, v\_j)\in\symbfscr{E}\;\text{かつ}\; \phi\_e((v\_i, v\_j))\symbfscr{G}eq t\right\\}. $$

 
 $\phi\_e((v\_i, v\_j))$ は，Definition
2.40で定義されるように，与えられたエッジをその関連する時点に対応付ける関数である．

</div>


この定義を踏まえると，時間的ランダムウォークは以下のように記述される．

<div class="definition">
 
<strong>定義 4.8 時間的ランダムウォーク</strong>

 $\symbfscr{G}=\left\\{\symbfscr{V},\symbfscr{E},\phi\_e\right\\}$ をダイナミックグラフとし， $\phi\_e$ をエッジに紐づく時間を返す関数である．ここでは， $(v^{(0)},v^{(1)})$ を最初のエッジとするノード $v^{(0)}$ を出発点とする時間的ランダムウォークを考える．
今 $k$ 番目のステップで，ノード $v^{(k-1)}$ からノード $v^{(k)}$ まで進んだところであると仮定し，今度はノード $v^{(k)}$ の時間的近傍 $\symscr{N}\_{(\phi\_e((v^{(k-1)},v^{(k)})))}(v^{(k)})$ の中から移動先のノードを以下の確率でに沿って選択する．
 $$ p(v^{(k+1)}|v^{(k)}) =  
    \begin{cases}
        \mathrm{pre}(v^{(k+1)}) & v^{(k+1)}\in \symscr{N}\_{(\phi\_e((v^{(k-1)},v^{(k)})))}(v^{(k)})\\
        0, & \text{otherwise}.
    \end{cases} $$ 
ここで， $\mathrm{pre}(v^{(k+1)})$ は以下のように定義され，現在時点との時間差が小さいノードが高確率で選択されることになる．
 

$$ \mathrm{pre}(v^{(k+1)}) = \dfrac{\exp\left\[\phi\_e((v^{(k-1)},v^{(k)}))-\phi\_e((v^{(k)},v^{(k+1)}))\right\]}{\displaystyle\sum_{v^{(j)}\in\;\symscr{N}\_{(\phi\_e((v^{(k-1)},v^{(k)})))}(v^{(k)})}\exp\left\[\phi\_e((v^{(k-1)}, v^{(k)}))-\phi\_e((v^{(k)}, v^{(j)}))\right\]}. $$

 

</div>


時間的ランダムウォークは，進むべき時間的近傍がないときに自然に終了する．そこで，DeepWalkのような固定長のランダムウォークを生成するのではなく，共起抽出のために必要な最低限のウィンドウサイズ $w$ と，予め定義された長さ $T$ までの間の長さの時間的ランダムウォークを生成することになる．
そして，これらのランダムウォークを利用して共起するノード組を生成し，DeepWalkと同じように情報の再構成を行う．


[メインページ](../../index.markdown)

[章目次](./chap4.md)

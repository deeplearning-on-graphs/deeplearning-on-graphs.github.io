[メインページ](../../index.markdown)
[章目次](./chap3.md)
## 3.5. オートエンコーダー

![オートエンコーダーは, 入力から出力までを記憶する.
太字の接続は入力から出力まで記憶することを示し,
他の接続はオートエンコーダでは使用しない（重み0）ことを示す.](./fig/fig3_15.png)

オートエンコーダーは,
入力を再現して出力しようとするニューラルネットワークと見なすことができる.
具体的には, 入力情報をエンコードした中間隠れ表現 $\mathbf{h}$ を持つ.
オートエンコーダーは2つの要素から構成される： 1) 入力
 $\mathbf{x}$ を符号化して符号化表現 $\mathbf{h}$ を生成する, エンコーダ部分
 $\mathbf{h} = f (x)$  と, 2) 符号化表現  $\mathbf{h}$  から  $\mathbf{x}$ 
を復元するデコーダ部分 $\hat{\mathbf{x}} = g (h)$ である.
もしオートエンコーダが入力を完全に再現することができるなら,それは使い物にならない.
むしろ, オートエンコーダーはいくつかの制限を入れた上で,
入力を近似的に再現するものである. より具体的には,
オートエンコーダは入力の必要な情報を符号化表現  $\mathbf{h}$  に圧縮して,
満足のいく出力を再現する. オートエンコーダの一般的な構成を図3.16に示す.
入力 $\mathbf{x}$ は「ボトルネック」を通過し,
ここでは符号化表現 $\mathbf{h}$ に保存できる情報が制御される. そして,
デコーダ部分 $\mathbf{h}$ を利用して入力 $\mathbf{x}$ を再構成した $\hat{\mathbf{x}}$ を出力する.
オートエンコーダーのネットワークは次の再構成誤差を最小化することで学習させることができる：
 

$$ \ell(\mathbf{x}, \hat{\mathbf{x}})=\ell(\mathbf{x}, g(f(\mathbf{x}))) $$

 
ここで,
 $\ell(\mathbf{x}, \hat{\mathbf{x}})$ は $\mathbf{x}$ と $\hat{\mathbf{x}}$ の違いを表す.

![オートエンコーダーの一般的な構成](./fig/fig3_16.png)

例えば, 平均二乗誤差を $l$ とすることができる. オートエンコーダーでは,
「ボトルネック」の設計が重要である. 図3.15に示すように,
「ボトルネック」がないと,
オートエンコーダは単に入力を記憶してデコーダにそのまま渡して入力を再現することを学習してしまい,
オートエンコーダーが使い物にならなくなる可能性がある.
「ボトルネック」を設計する（すなわち,
オートエンコーダーに制約を加える）方法はさまざまある. 自然な方法は,
符号化表現 $\mathbf{h}$ の次元数を制限することであり,
これは不完全オートエンコーダーの考え方につながる. また,
正則化項を追加して入力と出力間の記憶を抑止することもでき,
これは正則化オートエンコーダーの考え方につながる.

### 不完全オートエンコーダー

符号化表現 $\mathbf{h}$ の次元数が入力 $\mathbf{x}$ より小さくなるように制限することは,
「ボトルネック」を設計する上で, 簡単で自然な方法である.
入力次元より小さい符号化部分の次元を持つオートエンコーダーは,
「不完全」オートエンコーダーと呼ばれる.
不完全オートエンコーダーの例は図3.16に示されている.
エンコーダーとデコーダーはともに1層のネットワークしか持たず,
隠れ層は入力層より少ないユニットしか持っていない.
このモデルは再構成誤差を最小にすることで,
入力の最も重要な特徴を隠れ符号化表現に保存することができる.

### 正則化オートエンコーダー

また, エンコーダーとデコーダーの層を重ねることで,
オートエンコーダーをより深くすることができる.
深いオートエンコーダーの場合, その表現力の大きさに注意する必要がある.
オートエンコーダーは, エンコーダーとデコーダーの表現力が大きすぎると,
意味があることを何も学べなくなってしまう可能性がある.
オートエンコーダーが単に「恒等関数」を学習してしまうことを防ぐため,
オートエンコーダーの損失関数に正則化項を以下のように入れることができる：

 

$$ \ell(\mathbf{x}, g(f(\mathbf{x})))+\eta \cdot \Omega(\mathbf{h}) $$

 
ここで,  $\Omega(\mathbf{h})$ は正則化項で,
 $\eta$ は正則化項の大きさを制御するハイパーパラメータである.

(Olshausen and Field, 1997)の研究では,
符号化表現 $\mathbf{h}$ の $L_1$ ノルムが正則化項として次のように導入されていた：
 

$$ \Omega(\mathbf{h})=\|\mathbf{h}\|_1 $$

 
 $L_1$ ノルム正則化項によって $\mathbf{h}$ はスパースになるよう誘導される.
こうしたオートエンコーダーは「スパースオートエンコーダー」とも呼ばれる.

符号化表現をスパースにする別の方法は, $\mathbf{h}$ のニューロンがほとんどの時間で不活性であるように制約することである.
ここでいう「不活性」とは,
 $\mathbf{h}$ のニューロンの値が低いレベルであることを意味する.
ここまでは $\mathbf{h}$ を使って隠れ符号化状態を表現しているが,
これではどのような入力がこのコードにつながるのかが明示的にわからない.
そこで, 関係を明示的に表現するため, 与えられた入力 $\mathbf{x}$ に対して,
オートエンコーダーで学習したその符号化表現を $\mathbf{h}(x)$ とする.
サンプル集合 $\left\{\mathbf{X}_{(i)}\right\}_{i=1}^{m}$ の符号化表現の平均は次のように書くことができる：
 

$$ \overline{\mathbf{h}}=\frac{1}{m} \sum_{i=1}^{m} \mathbf{h}\left(\mathbf{x}_{(i)}\right) $$

 

次に, 隠れ符号化表現の各要素が小さな値 $\rho$ に近づくようにしたい.
例えば,  $\rho$ を0.05とする. (Ng et al., n.d.)の研究では,
隠れ表現の各要素を̄ $\overline{\mathbf{h}}$ を平均とするベルヌーイ確率変数として扱っている.
KLダイバージェンスを用いることで,
この確率変数が $\rho$ を平均とするベルヌーイ確率変数と近づくように制約することができる：
 $$ \Omega(\mathbf{h})=\sum_j\left(\rho \log \frac{\rho}{\overline{\mathbf{h}}[j]}+(1-\rho) \log \frac{1-\rho}{1-\overline{\mathbf{h}}[j]}\right)
    \tag{3.12} $$ 

式(3.12)の正則化項を持つオートエンコーダーも「スパースオートエンコーダー」と呼ばれる.
正則化項は, 「不完全オートエンコーダー」に適用することもできるが,
正則化項単体で「ボトルネック」の役割を果たすこともできる.
正則化項を用いれば,
隠れ符号化表現 $\mathbf{h}$ は必ずしも入力より小さい次元である必要はなくなる.


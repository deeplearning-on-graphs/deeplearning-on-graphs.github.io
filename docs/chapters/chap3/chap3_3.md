[メインページ](../../index.markdown)

# 3. 深層学習の基礎

## 3.1 はじめに
## 3.2 深層順伝播型ネットワーク
## 3.3 畳み込みニューラルネットワーク

CNN(Convolutional Neural Networks)は,
よく使われているニューラルネットワークの一種であり,
画像などの規則的な格子状のデータを処理するのに適している.
CNNは多くの点で順伝播型ニューラルネットワークと似ている.
学習可能な重みとバイアスを持つニューロンで構成されており,
各ニューロンは前の層からいくつかの情報を受け取って変換する.
CNNが順伝播型ニューラルネットワークと異なる場合があるのは,
CNNのいくつかのニューロンの構造である. より具体的には,
ニューロンを構成する際に畳み込み演算が導入されている点である.
このような畳み込み演算を行う層を畳み込み層と呼ぶ. 畳み込み演算は通常,
前の層の少数のニューロンの情報のみを使い,
層間の接続を疎にすることができる.
CNNにおけるもう一つの重要な操作はプーリング操作で,
これは近くのニューロンの出力をまとめて新しい出力とする.
プーリング演算を行う層をプーリング層と呼ぶ. この節では,
まず畳み込み演算と畳み込み層について紹介し,
次にプーリング層について議論し, 最後にCNNの全体的な枠組みを述べる.

### 3.3.1 畳み込み演算と畳み込み層

一般に, 畳み込み演算は,
2つの実関数に対して第3の関数を生成する数学的演算である（Widder and
Hirschman, 2015）.2 つの関数 $f () $ と  $g () $ の間の畳み込み演算は,
以下のように定義できる：

$$
(f * g)(t)=\int_{-\infty}^{\infty} f(\tau) g(t-\tau) d \tau \tag{3.1}
$$

畳み込み演算を具体的に理解するために, 連続的な信号 $f(t) $を考える.
ここで,  $t $は時間,  $f(t) $は時間 $t $における対応する値である.
この信号がいくらかノイズを含んでいるとする.
ノイズの少ない信号を得るために, 時刻 $t $の値とその近傍の値を平均化したい.
さらに,  $t $に近い時間帯の値は,  $t $の値と類似している可能性があり,
より多く寄与するはずである. そこで,
時刻 $t $に近いいくつかの値の加重平均を新しい値にする.
これは信号 $f(t) $と重み関数 $w(c) $の畳み込み演算としてモデル化することができる.
畳み込み演算後の信号は, 以下のように表すことができる：

$$
s(t)=(f * w)(t)=\int_{-\infty}^{\infty} f(\tau) w(t-\tau) d \tau
$$

なお, この演算が加重平均を行うことを保証するために,
 $w(c) $は積分して1になるように制約されているので,
 $w(c) $は確率密度関数となる. 一般には,
畳み込み演算は加重平均である必要はなく,
関数 $w(t) $はこれらの要件を満たす必要はない.

実際には, データは一定の間隔を持った離散的なものであることが多い.
例えば, 信号  $f (t) $ は時間  $t $
の整数値でしかサンプリングされないことがある. 先ほどの例の $f () $と
 $w() $がともに時間  $t $の整数値で定義されているとすると,
畳み込みは次のように書くことができる：

$$
s(t)=(f * w)(t)=\sum_{\tau=-\infty}^{\infty} f(\tau) w(t-\tau)
$$

さらに, ほとんどの場合,
関数 $w() $は小さなウィンドウ幅でしか非ゼロにならない. 言い換えれば,
局所的な情報のみが畳み込む場所の新しい値に寄与する.
ウィンドウ幅が $2n＋1 $, すなわち,
 $c＜n $と $c＞n $で $w（c）＝0 $であるとすると,
畳み込みはさらに次のように書き換えることができる.

$$
(f * w)(t)=\sum_{\tau=t-n}^{t+n} f(\tau) w(t-\tau)
$$

ニューラルネットワークの場合,
 $t $は入力層にあるユニットのインデックスと考えることができる.
関数 $w() $はカーネルまたはフィルタと呼ばれる.
畳み込み演算は疎結合のグラフとして表すことができる. 畳み込み層は,
カーネルを入力層上でスライドさせ,
それに対応する出力を計算すると解釈することができる. に,
畳み込み演算を構成する層の例を示す.

<figure id="fig:fig3_6">
<img src="chapters/chap3/fig/fig3_6.png" />
<figcaption>畳み込み層の例</figcaption>
</figure>

::: eg
は, 入力と出力が同じ大きさの畳み込み層である.
出力層の大きさを維持するために,
入力層には値0のユニットが2つ追加されている（破線の円）.
畳み込み演算のカーネルは図の右側に示されている. 簡単のため,
非線形活性化関数は図に示していない. この例では,  $n = 1 $ であり,
カーネル関数は近傍の 3 箇所でのみ定義されている.
:::

実際の機械学習では, 画像のような2次元以上のデータを扱うことが多い.
畳み込み演算は高次元のデータにも拡張可能である. 例えば,
2次元の画像 $I $に対して,
2次元のカーネル $K $を用いて以下のように畳み込み演算を行うことができる：
 $$S(i,  j)=(I * K)(i,  j)=\sum_{\tau=i-n}^{i+n} \sum_{j=\gamma-n}^{\gamma+n} I(\tau,  \gamma) K(i-\tau,  j-\gamma)\nonumber $$

次に, 畳み込み層の主要な特性について述べる.
一次元データに対する畳み込み層を考えたとしても一般性を失わない.
これらの特性は高次元データにも適用可能である. 畳み込み層は主に,
「スパースな接続」, 「パラメータの共有」,
「平行移動に対する同変性」という3つの重要な特性を持っている.

#### スパースな接続 {#スパースな接続 .unnumbered}

<figure id="fig:fig3_7">
<img src="chapters/chap3/fig/fig3_7.png" />
<figcaption>密な接続とスパースな接続</figcaption>
</figure>

従来のニューラルネットワークでは,
入力ユニットと出力ユニットの間の相互作用は行列で記述することができる.
この行列の各要素は,
各入力ユニットと各出力ユニット間の相互作用を記述する独立なパラメータである.
しかし, 畳み込み層においては層間の接続は疎になることが多い. は,
従来のニューラルネットワーク層と畳み込みニューラルネットワーク層の比較を示している.
この図では, 1つの出力ユニット $S_3 $と,
 $S_3 $に影響を与える対応する入力ユニットを強調して表示している. 明らかに,
左の密な結合の場合には,
1つの出力ユニットがすべての入力ユニットの影響を受けている. しかし,
右の畳み込みニューラルネットワーク層では,
出力ユニット $S_3 $は3つの入力ユニット $x_2 $,  $x_3 $,
 $x_4 $（ $S_3 $の「受容野」と呼ばれる）の影響を受けるだけである.
スパースな接続の大きな利点は,
計算効率を大きく向上させることができる点である.
従来のニューラルネットワークの層では,
 $N $個の入力ユニットと $M $個の出力ユニットがあるとき,
 $N\times M $個のパラメータが存在し,
この層の1回の計算量は $O(N\times M) $である. 一方で,
同じ数の入力と出力ユニットを持つ畳み込み層は,
カーネルサイズが $K $であるとき,  $K\times M $個のパラメータしか持たず,
計算量は $O(K\times M) $にまで減少する. なお,
ここでは「パラメータの共有」は考慮しない（次で議論する）. つまり,
畳み込みニューラルネットワークの計算は,
従来のニューラルネットワークの計算よりはるかに効率的になる.

#### パラメータの共有 {#パラメータの共有 .unnumbered}

前述したように, 畳み込み層には  $K \times M $ 個のパラメータが存在する.
しかし, 畳み込み層の「パラメータの共有」により,
この数はさらに減少させることができる. 「パラメータの共有」とは,
異なる出力ユニットに対して計算を行う際に,
同じパラメータセットを共有することである. 畳み込み層では,
すべての出力ユニットの値を計算するために同じカーネルが使用される.
このため, 当然ながらパラメータは共有されることになる. はその例であり,
同じ色の接続は同じパラメータを共有する. この例では,
カーネルサイズを3としているため, 3つのパラメータが存在することになる.
一般に, カーネルサイズが $K $の畳み込み層では, パラメータは $K $個になる.
従来のニューラルネットワーク層の $N \times M $個のパラメータと比較すると,
 $K $個というパラメータ数はかなり小さく,
結果として必要なメモリもはるかに抑えられる.

<figure id="fig:fig3_8">
<img src="chapters/chap3/fig/fig3_8.png" />
<figcaption>パラメータの共有</figcaption>
</figure>

#### 平行移動に対する同変性 {#平行移動に対する同変性 .unnumbered}

パラメータを共有する仕組みからは,
CNNのもう一つの重要な特性である「平行移動に対する同変性」が導かれる.
ある関数が入力の変化と同じように出力が変化する場合,
その関数は同変であるという. 具体的には, 関数  $f () $ は,
 $f (g(x)) = g( f (x)) $ のとき, 関数  $g() $ に対して同変である.
畳み込み演算の場合, 平行移動に対して同変であることは容易に確かめられる.
例えば, の入力ユニットを1ユニット右にシフトしたとしても,
出力は同じように1ユニット右にシフトした結果となる. この性質は,
ある特徴がどこに現れるかよりも,
それが現れるかどうかを重視するようなタスクにおいて重要となる. 例えば,
ある画像に猫が写っているかどうかを認識する場合,
画像中のどこに特徴があるかではなく,
猫が写っていることを示す重要な特徴があるかどうかが問題となる.
CNNのこのような平行移動に対する同変性という特性は,
画像分類の分野で成功するために極めて重要である（Krizhevsky et al.2012;
He et al.2016)．

### 3.3.2 実際の畳み込み層

実際には, CNNにおける畳み込みについて議論するとき,
数学的に定義されているような厳密な畳み込み演算を考えることはない.
実際に使われる畳み込み層は数学的な定義とは若干異なる.
入力は実数値だけではなく, ベクトル値である場合が多い. 例えば,
 $N \times N $画素からなるカラー画像では, 各画素に赤, 緑,
青それぞれの強度を表す3つの値が割り当てられている.
各色は入力画像の「チャンネル」を表す.
一般に，入力画像のi番目のチャンネルは,
全ての入力ベクトルのi番目の要素で構成される.
各位置（例えば画像の場合は画素）におけるベクトルの長さがチャンネル数となる.
したがって, 畳み込みは通常3次元で行われるが,
そのうち2次元では「スライドする」だけである（つまり,
チャンネル方向の次元には畳み込みは行われない=「スライドしない」）.
さらに, 典型的な畳み込み層では, 入力層から特徴を抽出するために,
複数の異なるカーネルが並列に適用される. その結果,
出力層も複数チャンネル存在することになる.
ここでは各カーネルの結果が各出力チャンネルに対応することになる.
 $L $個のチャンネルを持つ入力画像 $I $を考えてみよう.
 $P $個のカーネルを用いた畳み込み演算は次のように定式化することができる：
 $$S(i, j, p)=\left(I * K_{p}\right)(i, j)=\sum_{l=1}^{L} \sum_{\tau=i-n}^{i+n} \sum_{j=\gamma-n}^{\gamma+n} I(\tau, \gamma, l) K_{p}(i-\tau, j-\gamma, l), p=1, \ldots P
    \label{eq:3_2} $$
ここで $K_p $は $p $番目のカーネルで、 $(2n+1)^2\cdot L $個のパラメータを持つ。
出力は $P $チャンネルから成る。

多くの場合, 計算量をさらに減らすため,
入力上でカーネルをスライドさせるときに,
いくつかの位置を規則的にスキップすることができる.
この畳込みは $s $番目ごとに行われ,  $s $を通常「ストライド」と呼び,
このようなストライドを持つ畳み込みをストライド付き畳み込みと呼ぶ.
例（ストライド $s $が2の場合）を(a)に図示した. ストライド付き畳み込みは,
(b)に図示したように,
通常の畳み込みの結果をダウンサンプリングしたものと見なすこともできる.
ストライド  $s $
のストライド付き畳み込みは，次のように表現することができる：
 $$\begin{array}{l}S(i, j, p)= \\ \sum_{l=1}^{L} \sum_{\tau=i-n}^{i+n} \sum_{j=\gamma-n}^{\gamma+n} I(\tau, \gamma, l) K_{p}((i-1) \cdot s+1-\tau,(j-1) \cdot s+1-\gamma, l)\end{array}\nonumber $$

<figure id="fig:fig3_9">
<img src="chapters/chap3/fig/fig3_9.png" />
<figcaption>ストライド付き畳み込みの概念図</figcaption>
</figure>

ストライドを  $s = 1 $
とすると，[\[eq:3_2\]](#eq:3_2){reference-type="eqref"
reference="eq:3_2"}のようにストライドなし畳み込みと等価になる．
前述したように, 通常は出力の大きさを維持するために,
入力にゼロパディングが施される. パディングの大きさ,
受容野の大きさ（またはカーネルの大きさ）, そしてストライドによって,
（入力サイズが固定である場合の）出力サイズが決定される. 具体的には,
サイズ $N $の1次元入力を考え, パディングのサイズを $Q $,
受容野のサイズを $F $,
ストライドのサイズを $s $とすると、出力のサイズ $O $は以下の式で計算することができる：
 $$O=\frac{N-F+2 Q}{s}+1
    \label{eq:3_3} $$

::: eg
のストライド付き畳み込みの入力サイズは $N=5 $である。
カーネルサイズは $F=3 $で, ゼロパディングは $Q=1 $である.
ストライドが $s=2 $であることから,
[\[eq:3_3\]](#eq:3_3){reference-type="eqref"
reference="eq:3_3"}を用いて出力サイズは次のように計算される：
 $$O=\frac{N-F+2 Q}{s}+1=\frac{5-3+2 \times 1}{2}+1=3\nonumber $$
:::

### 3.3.3 検出層

順伝搬型ネットワークと同様に,
畳み込み演算の後に非線形活性化関数が適用される.
CNNで広く用いられている活性化関数はReLU関数である.
非線形活性化を適用するプロセスは, 検出ステージまたは検出層とも呼ばれる.

### 3.3.4 プーリング層

畳み込み層と検出層の後に, 通常はプーリング層が続く. プーリング関数は,
近くの統計量をまとめて出力する. 従って, プーリング層の後では,
データの幅と高さが小さくなる. しかし,
データの深さ（チャンネル数）は変化しない.
一般的に使用されるプーリングには. に示すように,
最大値プーリングと平均値プーリングがある.
これらのプーリング演算は $2\times2 $の局所近傍を入力とし,
それらに基づいて1つの値を出力する. その名前が示すように,
最大値プーリングは局所近傍の最大値を出力とし,
平均値プーリングは局所近傍の平均値を出力とする.

<figure id="fig:fig3_10">
<img src="chapters/chap3/fig/fig3_10.png" />
<figcaption>CNNにおけるプーリング</figcaption>
</figure>

### 3.3.5 CNNフレームワークの全体像 {#sec:3_3_5}

畳み込み演算とプーリング演算を説明したので,
次に分類問題に対する畳み込みニューラルネットワークの全体的な枠組みについて述べる.
に示すように, 分類のための全体的な枠組みは, 特徴抽出パートと分類パートの
2 つに大別される. 特徴抽出パートは, 畳み込み層とプーリング層からなり,
入力から特徴を抽出する. 一方で,
分類パートは全結合の順伝播型ニューラルネットワークで構成されている.
これら2つの要素をつなぐのが平坦化処理である.
特徴抽出パートで抽出された複数チャネルの特徴量行列を1つの特徴量ベクトルに平坦化し,
分類パートの入力とする. では, 1 つの畳み込み層と 1
つのプーリング層しか描かれていないが,
実際には複数の畳み込み層とプーリング層を重ねるのが一般的である. 同様に,
分類パートにおいても順伝播型ニューラルネットワークは複数の全結合層で構成される場合がある.

<figure id="fig:fig3_11">
<img src="chapters/chap3/fig/fig3_11.png" />
<figcaption>CNNにおけるプーリング</figcaption>
</figure>


<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\ $','\ $'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

[メインページ](../../index.markdown)

# 3. 深層学習の基礎
## [3.1 はじめに](./chap3_1.md)

## 3.2 深層順伝播型ネットワーク

順伝播型ネットワークは, 多くの重要な深層学習技術の基礎となっている.
このネットワークは与えられたデータを用いて関数 $f^{\ast}(x) $を近似する.
例えば分類問題において,
理想的な分類器である $f^{\ast}(x) $は与えられた入力 $\mathbf{x} $を正解の分類 $\mathbf{y} $に対応づける.
この場合,
順伝播型ネットワークでは理想的な分類器 $f^{\ast}(x) $をうまく近似できるような写像 $f(\mathbf{x} \mid \Theta) $を見つける.
具体的には, 順伝播型ネットワークを学習する目的は,
 $f^{\ast}(x) $の最良の近似値を得ることができるパラメータ $\Theta $の値を学習するということになる.

順伝播型ネットワークでは,
情報 $\mathbf{x} $が入力からいくつかの中間計算を経て,
最終的に出力 $\mathbf{y} $に流れる.
これらの途中過程はネットワークの形をしており,
通常いくつかの関数の組み合わせとして表現される. 例えば,
の順伝播型ネットワークは,
4つの関数 $f^{(1)}, f^{(2)}, f^{(3)}, f^{(4)} $が鎖状に接続されており,
これらの関数を用いて $f(\mathbf{x}) $は $f(\mathbf{x})=f^{(4)}\left(f^{(3)}\left(f^{(2)}\left(f^{(1)}(\mathbf{x})\right)\right)\right) $と表される.
の順伝播型ネットワークでは,  $f^{(1)} $が第1層,  $f^{(2)} $が第2層,
 $f^{(3)} $が第3層, そして最後の層 $f^{(4)} $が出力層となっている.
ネットワークの計算層の数でネットワークの深さが決まる.
ニューラルネットワークは, 出力 $f(x) $を理想的な出力,
すなわち $f^{\ast}(x) $またはyに近づけようと学習する. 学習過程において,
出力層からの結果は直接学習データと比較される一方で,
全ての中間層はそうではない. したがって,
理想的な関数 $f^{\ast}(x) $をうまく近似するためには,
出力層から伝搬してくる間接的な情報を用いて中間層のパラメータを決定する.
中間層は, 学習時には学習データから望ましい出力が得られないため,
隠れ層と呼ばれている. 前述のように,
ニューラルネットワークの各層は入力と出力の両方がベクトルであるベクトル値関数とみなすことができる.
層の要素はノード(またはユニット)として扱うことができる. このように,
各層は,
各ノードが関数であるベクトルをスカラーに対応させる関数の集まりと考えることができる.
このようなネットワークは,
神経科学の言葉を借りてニューラルネットワークと呼ばれている.
ノードでの操作は, 脳のニューロンで起きていることを真似ており,
十分な刺激を受けたときに活性化する. ノードは,
前の層のすべてのノードから情報を集めて変換し, 活性化関数に送し,
次の層に情報をどの程度まで通過させるかを決める.
情報の収集と変換の操作は一般的に線形だが,
活性化関数はニューラルネットワークに非線形性を加え,
これによって近似能力が大きく向上する.

<figure id="fig:FFN">
<img src="./fig/fig3_1.png" width="75%" />
<figcaption>図3.1 順伝播型ネットワークの例</figcaption>
</figure>

### 3.2.1 ネットワークの構成

全結合型の順伝播型ニューラルネットワークでは，連続した層のノードは完全な二部グラフを形成している.
は，このネットワーク構成の全体像を表す. 次に,
ニューラルネットワークで行われる計算の詳細を述べる. ここでは,
第1層の1つのノードに注目する.
ニューラルネットワークの入力は，ベクトル $\mathbf{x} $で， $x_i $はそのi番目の要素を表す.
これらの要素はすべて入力層のノードとみなすことができる.
第2層のノード(または入力層の次の層のノード)は，入力層のすべてのノードに接続されている.
これらの入力層のノードと第2層の任意のノードとの接続をに示す.

<figure id="fig:fig3_2">
<img src="./fig/fig3_2.png" width="75%"/>
<figcaption>図3.2 ノードでの操作</figcaption>
</figure>

1つのノードにおける演算は, 次の2つの部分から構成される:

1.  入力を重み( $\mathbf{w}_{i}$ )で線形に組み合わせる

2.  前のステップで得られた値を, 活性化関数に通す

これは数学的には次のように書くことができる：

$$
h=\alpha\left(b+\sum_{i=1}^{4} \mathbf{w}_{i} \cdot \mathbf{x}_{i}\right)\nonumber
$$

ここで,  $b $はバイアス項,  $\alpha $は活性化関数（次の章で述べる）を表す.
次にこの演算を任意の隠れ層に一般化する.
ニューラルネットワークのk番目の層には $N^{(k)} $ノードあり,
その出力は $\mathbf{h}^{(k)} $と表すことができるとする（ここで,
 $\mathbf{h}^{(k)}_{i} $はi番目の要素を表す）.
ニューラルネットワークの $k+1$番目の層で $\mathbf{h}^{(k+1)} $を計算するために次の演算を行う：

$$
\mathbf{h}_{j}^{(k+1)}=\alpha\left(b_{j}^{(k)}+\sum_{i=1}^{N^{(k)}} \mathbf{W}_{j i}^{(k)} \mathbf{h}_{i}^{(k)}\right)\tag{3.1}
$$

ここで,
 $\mathbf{W}_{j i}^{(k)}$ は 
 $\mathbf{h}_i^{(k)}$ と 
 $\mathbf{h}_i^{(k+1)}$ をつなげる重みに対応し,
 $b_j^{(k)}$ はバイアス項を表す.
 $k+1$ 層の全要素を計算する演算を行列形式で書くと次のようになる：


$$
\mathbf{h}^{(k+1)}=\boldsymbol{\alpha}\left(\mathbf{b}^{(k)}+\mathbf{W}^{(k)} \mathbf{h}^{(k)}\right)\nonumber 
$$

ここで,
 $\mathbf{W}^{(k)} \in \mathbb{R}^{N^{(k+1)} \times N^{(k)}} $は全てのウェイトを含み,
(j, i)要素は式(3.1)の $\mathbf{W}_{j i}^{(k)} $である.
 $\mathbf{b}^{(k)} $は全てのバイアス項を含む. 特に,
入力層においては $\mathbf{h}^{(0)}=\mathbf{x} $となる.
ニューラルネットワークのk+1層目の演算を表すのに $f^{k+1} $を使っていたことを思い出すと,

$$
\mathbf{h}^{(k+1)}=f^{(k+1)}\left(\mathbf{h}^{(k)}\right)=\alpha\left(\mathbf{b}^{(k)}+\mathbf{W}^{(k)} \mathbf{h}^{(k)}\right)\nonumber 
$$

と書くことができる.
ここで紹介した演算は隠れ層において典型的なものである.
出力層も同様の構造であるが,
得られた情報を変換するための活性化関数が異なっていることが多い. 次に,
活性化関数と出力層の設計について述べる.

### 3.2.2 活性化関数

活性化関数は, 入力信号を通過させるかどうか, あるいはどの程度通過させ
るかを決定する. ノード(またはニューロン)は,
そこを通過する情報があれば活性化される. 前節で紹介したように,
活性化関数がない場合には, ニューラルネットワークの演算は線形になる.
活性化関数は, ニューラルネットワークに非線形性を導入し,
それによって近似能力が向上する. 以下では,
よく使われている活性化関数を紹介する.

#### ReLU関数

ReLU関数は最もよく使われる活性化関数の一つである. に示すように,
ReLU関数は線形関数と似ているが,
入力が負のときに0を出力する点が唯一異なる. ニューラルネットワークでは,
この活性化関数を採用したユニットをRectifier Linear Unit (ReLU)と呼ぶ.
ReLU関数はすべての正の入力に対して線形（というより同じ値を出力する）であり,
すべての負の入力に対して0である. 数学的には次のように定義される：

$$
\operatorname{ReLU}(z)=\max \{0,  z\}\nonumber
$$

<figure id="fig:fig3_3">
<img src="./fig/fig3_3.png" width="75%" />
<figcaption>図3.3 ReLU関数</figcaption>
</figure>

各層では, いくつかのユニットのみが活性化されるため, 効率的な計
算が可能になる. ReLU関数の欠点は,
入力が負の領域で勾配が0になることである. したがって,
ユニットが活性化されないと,
そのユニットを訓練するための情報が伝搬しなくなってしまう.
この欠点を克服するために, ReLU関数のいくつかの一般化が提案されている.
LeakyReLU関数 (Maas et al., 2013)は, 負の入力を0にする代わりに, (a)に
示すように, 負の値に対して小さな傾きを持つ線形変換を行う.
より具体的には, LeakyReLU関数は数学的に次のように表すことができる：

$$
\operatorname{LeakyReLU}(z)=\left\{\begin{array}{cc}0.01 z & z<0 \\ z & z \geq 0\end{array}\right.\nonumber
$$

<figure id="fig:fig3_4">
<img src="./fig/fig3_4.png" width="75%"/>
<figcaption>図3.4 ReLU関数の一般化</figcaption>
</figure>

ReLU関数をさらに一般化したものが, ELU関数 (Exponential Linear
Unit)である. ELU関数は(b)に示すように, 正の値に対しては恒等変換を行うが,
負の値に対しては指数関数的な変換を行う. 数学的には,
ELUの活性化関数は次のように表される：

$$
\operatorname{ELU}(z)=\left\{\begin{array}{cc}c \cdot \exp (z-1) & z<0 \\ z & z \geq 0\end{array}\right.\nonumber
$$

ここで,  $c $は負の入力に対して指数関数の傾きを決める正の定数である.

#### ロジスティックシグモイド関数とtanh関数 

活性化関数としては,
ReLU関数よりも先にロジスティックシグモイド関数とtanh関数が最もよく使われてきた.
ロジスティックシグモイド関数は数学的には次のように表される：

$$
\sigma(z)=\frac{1}{1+\exp (-z)}\nonumber
$$

<figure id="fig:fig3_5">
<img src="./fig/fig3_5.png" width="75%" />
<figcaption>図3.5 ロジスティックシグモイド関数とtanh関数</figcaption>
</figure>

(a)に示すように, シグモイド関数は入力を0から1の範囲に変換する.
具体的には, 入力が負の値であるほど出力は0に近づき,
入力が正の値であるほど出力は1に近づくことになる.

tanh関数はシグモイド関数と次のような関係がある：

$$
\tanh (z)=\frac{2}{1+\exp (-2 z)}-1=2 \cdot \sigma(2 z)-1\nonumber
$$

(b)に示すように, tanh関数は入力を-1から1の範囲に変換する. 具体的には,
入力が負の値であるほど出力は-1に近くなり,
入力が正の値であるほど出力は1に近づく.

この2つの活性化関数は,
サチュレーション問題という同じ問題に直面している(Nwankpa et al.).
入力 $z $が巨大な正の数または負の数である場合に出力が一定に近づいてしまう.
つまり, この活性化関数は0に近い値にしか感度がない.
 $z $が巨大な正の数または負の数である場合には勾配が0付近になるため,
勾配ベースの学習が非常に難しくなる. このような理由から,
この2つの活性化関数は順伝搬型ネットワークではあまり使われなくなってきている.

### 3.2.3 出力層と損失関数

出力層と損失関数は, どのようなタスクに適用するかによって異なる. 次に,
一般的に使用される出力ユニットと損失関数を紹介する.

回帰問題では, ニューラルネットワークは連続値を出力する必要がある.
これを実現する簡単な方法は, 非線形活性化を行わずに,
アフィン変換を行うことである.
入力(または前の層からの情報)が $\mathbf{h} \in \mathbb{R}^{d_{i n}} $であるとき,
線形ユニットの層は,
ベクトル $\hat{\mathbf{y}} \in \mathbb{R} d_{o u} $を次のように出力する：

$$
\hat{\boldsymbol{y}}=\boldsymbol{W h}+\boldsymbol{b}\nonumber
$$

ここで,
 $\mathbf{W} \in \mathbb{R}^{d_{o u} \times d_{i n}} $と $\mathbf{b} \in \mathbb{R}^{d_{o u}} $は学習すべきパラメータである.
一つのサンプルに対して,
予測値 $\hat{\boldsymbol{y}} $と正解 $\boldsymbol{y} $との差を測定するために,
以下のように単純な二乗損失関数を用いることができる：

$$
\ell(\mathbf{y},  \hat{\mathbf{y}})=(\mathbf{y}-\hat{\mathbf{y}})^{2}\nonumber
$$

分類問題では,
ニューラルネットワークは与えられたサンプルのクラスの情報が必要になる.
与えられたサンプルの予測ラベルにあたる離散的な値をを直接出力する代わりに,
通常は, ラベルに対する離散的な確率分布を出力する.
予測対象が二値か多値かによって, 異なる出力層と損失関数が使われる. 次に,
この2つの場合の詳細について述べる.

#### 二値分類

2値分類のタスクでは,
サンプルは0か1のどちらかにラベル付けされていると仮定する.
そして予測を行うために,
まず入力（前の層からの結果）を一つの値に変換する線形層が必要である.
続いて, シグモイド関数を適用して, この値を0から1の範囲に対応させる.
これはサンプルがラベル1として予測される確率に対応する. まとめると,
この処理は次のようにモデル化できる：

$$
\hat{y}=\sigma(\mathbf{W h}+b)\nonumber
$$ 

ここで,
 $\mathbf{h} \in \mathbb{R}^{d_{i n}} $および $\mathbf{W} \in \mathbb{R}^{1 \times d_{i n}} $である.
特に,  $\hat{y} $は入力サンプルがラベル1と予測される確率を示し,
 $1-\hat{y} $はラベル0と予測される確率を表す. 出力 $\hat{y} $を用いると,
交差エントロピー損失関数を用いることで,
正解値と予測値の差を次のように計算することができる：

$$
\ell(y,  \hat{y})=-y \cdot \log (\hat{y})-(1-y) \cdot \log (1-\hat{y})\nonumber
$$

モデルを用いて推論する際には,
 $\hat{y}>0.5 $の時に入力サンプルはラベル1と予測され,
そうではない時にラベル0と予測される.

#### 多値分類

多クラス(nクラス）の分類課題では,
正解はは0からn-1までの整数で表されると仮定する. そこで,
分類ラベルを表現するためにOne-hotベクトル $\mathbf{y} \in\{0, 1\}^{n} $を用いる.
ここで $\mathbf{y}_{i}=1 $はサンプルが $i-1 $とラベル付けされていることを表す.
予測を行うために,
まず入力 $\mathbf{h} $をn次元ベクトル $\mathbf{Z} \in \mathbb{R}^{n} $に変換する線形層が必要である：

$$
\mathbf{z}=\mathbf{W}\mathbf{h}+\mathbf{b}\nonumber
$$ 

ここで,
 $\mathbf{W} \in \mathbb{R}^{n \times d_{i n}} $および $\mathbf{b} \in \mathbb{R}^{n} $である.
次に, ソフトマックス関数に入力し,
 $z $を全クラスに渡って離散的な確率密度関数に標準化する：
 
$$
\hat{\mathbf{y}}_{i}=\operatorname{softmax}(z)_{i}=\frac{\exp \left(\mathbf{z}_{i}\right)}{\sum_{j} \exp \left(\mathbf{z}_{j}\right)},  i=1,  \ldots,  n\nonumber
$$

ここで,  $\mathbf{z}_{i} $はベクトル $z $のi番目の要素を表し,
 $\hat{\mathbf{y}}_i $はソフトマックス関数のi番目の出力を表す. 特に,
 $\hat{\mathbf{y}}_i $は入力サンプルがラベル $i-1 $で予測される確率を表す.
予測値
 $\hat{\mathbf{y}} $に対して，交差エントロピー損失関数を用いることで,
正解値と予測値の差を計算することができる：

$$
\ell(\mathbf{y},  \hat{\mathbf{y}})=-\sum_{i=0}^{n-1} \mathbf{y}_{i} \log \left(\hat{\mathbf{y}}_{i}\right)\nonumber
$$

モデルを用いて推論する際には,
 $\hat{\mathbf{y}}_i $が $\hat{\mathbf{y}} $のn個の要素の中で最も大きい時に入力サンプルはラベル $i-1 $と予測される.


## [3.3 畳み込みニューラルネットワーク](./chap3_3.md)
## 3.4 リカレントニューラルネットワーク
## 3.5 オートエンコーダー
## 3.6 深層ニューラルネットワークの学習
## [3.7 本章のまとめ](././chap3_7.md)
## [3.8 参考文献](././chap3_8.md)


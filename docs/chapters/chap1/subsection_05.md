[メインページ](../../index.markdown)

[章目次](./chap1.md)
## 1.5. グラフの特徴の学習の歴史

上で言及したように，グラフを使った計算タスクで従来の機械学習を活用するには，ノードのベクトル表現を見つける必要がある．
この目的を達成するための主要な方法は，特徴量エンジニアリングと特徴量学習の2つが存在する(図1.5)．

<figure>

<img src="./fig/fig1_5.png" width="100%"/>

<figcaption>図1.5 ノード特徴量の抽出</figcaption>

</figure>

特徴量エンジニアリングは，ノードの次数に関する統計量などの手作業で設計された特徴量に依存するが，特徴量学習は，ノードの特徴量を自動的に学習させるものである．
下流のタスクにとって重要な特徴量がどのようなものか予備知識を持っていないことが多いため，特徴量エンジニアリングによる特徴量は下流タスクに最適でない可能性がある．
特徴量エンジニアリングのプロセスは膨大な人手が必要である．

一方で，特徴量学習は特徴量を自動的に学習するものであり，下流タスクに沿って学習プロセスが進められる．
その結果，学習された特徴量は下流タスクに適している可能性が高く，特徴量エンジニアリングによる特徴量と比べて優れた性能が得られることが多い．
さらに，このプロセスは人間の介入を最小限に抑え，新しいタスクへ容易に適応できる．
それゆえ，グラフにおいては特徴量学習が積極的に研究され，さまざまな場面や要求に対応するために，多数の手法が提案されている．

グラフにおける特徴量学習に関する手法は，特徴量選択と表現学習に大別できる．
前者は無関係で冗長なノード特徴量を排除することを目指し，後者は新しいノード特徴量の集合を生成することを目指す．
本節では，グラフ深層学習を理解するための一般的で歴史的な文脈を読者に提供する目的で，これら2つの手法グループを簡潔に解説していく．

### グラフにおける特徴量選択

現実世界のデータは高次元であり，特定のタスクを考慮する場合には，ノイズや無関係なもの，冗長な特徴量（または次元）などが含まれていることが多い．
特徴量選択の目的は，冗長性が最小で目的変数(例えば，教師ありデータにおけるクラスのラベル)との関連性は最大となるような，小さな特徴量の集合を自動的に選択することである．

多くの応用例では，知識を抜き出したりモデルを解釈する際，元々の特徴量が重要である．
例えば，がん研究のための遺伝子解析では，がん組織の鑑別に加えて，がん化を誘発する遺伝子（すなわち，元の特徴量）を特定することがより重要である．
このような要請の厳しい応用例では特徴量選択が特に好まれる．
なぜならこの方法は，元の特徴量を維持し，その意味内容が通常，学習課題に対して重要な洞察を提供するためである．
従来の特徴量選択では，各データが独立かつ同一に分布している（i.i.d.）と仮定している．
しかしながら，多くの応用例におけるデータサンプルは本質的にi.i.d.ではないようなグラフに埋め込まれている．
そのため，グラフにおける特徴量選択が研究されてきた．

 $\mathcal{V}$ をノードの集合， $\mathcal{E}$ をエッジの集合として，グラフ $\mathcal{G}=\{\mathcal{V}, \mathcal{E}\}$ を考える．
各ノードは $d$ 個の特徴量から成る集合 $\mathcal{F}=\left\{f\_1, f\_2, \ldots, f\_d\right\}$ を持つとする．
グラフにおける特徴量選択の目的は， $\mathcal{F}$ から各ノードを表現するような $K$ 個（ $K$ は $d$ よりも十分小さい）の特徴量を選ぶことである．
この問題は，Gu and Han (2011)およびTang and Liu
(2012a)で初めて，教師あり学習の設定で研究された．
この研究では，正則化項を導入した線形分類器を用いることでグラフの構造情報を反映した特徴量を選択し，それらの特徴量からクラスラベルへのマッピングを行っている．
この正則化項は，つながりのあるノードが，選択された特徴量を用いて類似のラベルにマッピングされることを保証するために導入されている．
その後，この問題は教師なし学習の設定の下でさらに研究された(Wei *et al*.,
2015, 2016; Tang and Liu, 2012b)． Tang and Liu
(2012b)では，構造情報から抽出した擬似ラベルを，特徴量選択プロセスでの指針として活用している．
Wei *et al*.
(2016)では，ノードの(情報や属性といった)内容と構造情報の両方が，高品質の特徴量集合から生成されると仮定しており，その特徴量集合はデータ生成プロセスの尤度を最大化することによって得られるとしている．
その後，この問題は単純グラフだけでなく，動的グラフ(Li *et al*.,
2016)や，多次元グラフ(Tang *et al*., 2013b)，符号付きグラフ(Cheng *et
al*., 2017; Huang *et al*., 2020)，属性グラフ(Li *et al*.,
2019b)などの複雑グラフに拡張されている．

### グラフにおける表現学習

グラフにおける特徴量選択とは異なり，グラフにおける表現学習は新しいノードの特徴量を学習することを目的としている．
この研究は何十年にもわたって広範囲に行われており，深層学習の登場によって劇的に加速した．
この小節では，「浅い」モデルから深層モデルまでの歴史的概要を簡単に説明する．

はじめの段階では，グラフにおける表現学習は，スペクトラルクラスタリング(Shi
and Malik, 2000; Ng *et al*., 2002)や，グラフを用いた次元削減 (Belkin
and Niyogi, 2003; Tenenbaum *et al*., 2000; Roweis and
Saul，2000)，行列因子分解 (Zhu *et al*., 2007; Tang *et al*., 2013a;
Koren *et al*., 2009）の文脈で研究されてきた．

スペクトラルクラスタリング（Shi and Malik, 2000; Ng *et al*.,
2002）では，データ点はグラフのノードと見なされ，クラスタリングはノードから成るコミュニティにグラフを分割することに対応する．
スペクトラルクラスタリングの重要なステップの1つは，スペクトラル埋め込みである．
これは，(k-meansなど)従来のクラスタリングアルゴリズムが適用できるような低次元空間に，ノードを埋め込むことを目的としている．

グラフを用いた次元削減技術は，ノード表現を学習するために直接利用することができる．
このアプローチは通常，データサンプルの元の特徴を使用してあらかじめ定義した距離（または類似性）関数を用いて親和性グラフを構築することになる．
そして，この親和性グラフの構造情報を保持するノード表現を学習する．
例えば，IsoMap (Tenenbaum *et al*.,
2000)は各データサンプル間をつなぐ距離を通じて全体の幾何構造を保持することを目的としており，
一方でLLE (Roweis and Saul, 2000)とeigenmap (Belkin and Niyogi,
2003)は，親和性グラフ内の局所的な近傍を保持することを目指している．

以上の方法は，親和性行列（または隣接行列やラプラシアン行列）上で固有値分解を実行する必要があることが多いため，計算コストが高くなりがちである．しかし，隣接行列，接続行列，ラプラシアン行列など，行列はグラフを表すための最も一般的なアプローチの1つである．そこで，行列因子分解をノード表現の学習に適用することを考える．
例えば，隣接行列を用いてグラフを表す場合，目的は低次元空間に埋め込んだノード表現を用いて隣接行列を再構成することである．

行列因子分解については文書コーパスと推薦システムに対する応用例が存在する．
文書コーパスは，文書と単語をノードとした二部グラフを使って表すことができ，ある単語文書にが現れる場合にその単語と文書の間にエッジが生じる．意味的情報検索における潜在意味インデックス(LSI;
Latent Semantic
Indexing)という手法は，文書と単語の表現を学習するために特異値分解(SVD)による次元圧縮を使っている
(Deerwester *et al*., 1990)．
一方，推薦システムでは，ユーザーとアイテム間の相互作用は二部グラフとして捉え，行列因子分解を用いてユーザーとアイテムの表現を学習し，推薦に活用している
（Koren *et al*., 2009）． 行列因子分解は他にも，ノード分類 (Zhu *et
al*., 2007; Tang *et al*., 2016a)やリンク予測 (Menon and Elkan, 2011;
Tang *et al*., 2013a)，コミュニティ検出 (Wang *et al*.,
2011)などのタスクにおいて，ノード表現の学習に活用されている．
また，本書の第4章で紹介するグラフ埋め込みアルゴリズムも，行列因子分解の枠組みとしてまとめることができる
(Qiu *et al*., 2018b)．

Word2vecは，単語のベクトル表現（埋め込み）を生成する手法である (Mikolov
*et al*., 2013)．
大規模なテキストコーパスを入力として受け取り，コーパス内の各単語に対してベクトル表現を生成する．
Word2vecが様々な自然言語処理タスクで大成功を収めたことから，グラフ領域でのノード表現学習に対しては，特にSkip-gramモデル[^4]
を適用する試みが増えている． [^4]: 訳注：Skip-gramモデルはWord2vecのアルゴリズムの1種であり，ある単語からその周辺の複数ある単語を推測するモデルである．
DeepWalk (Perozzi *et al*.,
2014)は，この目標を達成するための第一歩となった．
具体的には，与えられたグラフ内のノードは単語として扱われ，ノード列（文章）はランダムウォークによって生成される．
そして，生成したランダムウォーク内で共起するノードを保存するノード表現を学習するために，Skip-gramモデルが使用されている．
その後，多くの研究が3つの主要な方向性で展開されている：

(1) ノード共起を保持するための高度な方法を開発する方向性 (Tang *et al*.,
    2015; Grover and Leskovec, 2016; Cao *et al*., 2015)

(2) ノードの構造的役割 (Ribeiro *et al*., 2017)や，コミュニティ情報
    (Wang *et al*., 2017c)，ノードの状態 (Ma *et al*., 2017; Lai *et
    al*., 2017; Gu *et al*., 2018)
    などといった種類の情報を保存する方向性．

(3) 有向グラフ (Ou *et al*., 2016)，へテログラフ (Chang *et al*., 2015;
    Dong *et al*., 2017)，二部グラフ (Gao *et al*., 2018b)，多次元グラフ
    (Ma *et al*., 2018d)，符号付きグラフ (Wang *et al*.,
    2017b)，ハイパーグラフ (Tu *et al*., 2018)，動的グラフ (Nguyen *et
    al*., 2018; Li *et al*.,
    2017a)などの複雑グラフに対するフレームワークを設計する方向性

表現学習における，深層ニューラルネットワーク（DNN）の威力と成功を受けて，DNNをグラフに一般化する取り組みが増えてきている．
この方法は，グラフニューラルネットワーク（GNN）として知られており，空間的アプローチとスペクトルアプローチに大別できる．
空間的アプローチは，空間的に近い近傍ノードなど，グラフ構造を明示的に利用する．
このアプローチは，Scarselli *et al*. (2005)で初めて紹介された．
一方，スペクトルアプローチは，グラフフーリエ変換と逆グラフフーリエ変換を活用することで，グラフのスペクトル的な観点を活用する(Bruna
*et al*., 2013)．
深層学習の時代において，GNNは以下の側面で急速に発展している．

-   スペクトルアプローチ(Bruna *et al*., 2013; Defferrard *et al*.,
    2016; Kipf and Welling, 2016a)や空間アプローチ(Atwood and Towsley,
    2016; Niepert *et al*., 2016; Gilmer *et al*., 2017; Monti *et al*.,
    2017; Veličković *et al*., 2017; Hamilton *et al*.,
    2017a)を含め，数多くの新しいGNNモデルが登場した．

-   グラフ分類のようなグラフに焦点を当てたタスクでは，グラフ全体の表現が求められる．
    そこで，ノード表現からグラフ表現を得るための多数のプーリング手法が導入されている
    (Li *et al*., 2015; Ying *et al*., 2018c; Gao and Ji, 2019; Ma *et
    al*., 2019b)．

-   伝統的なDNNは敵対的攻撃に対して脆弱であるが，
    GNNもこの欠点を引き継いでしまっている．
    様々なグラフ敵対的攻撃が研究されており (Zug̈ner *et al*., 2018;
    Zug̈ner and Gun̈nemann, 2019; Dai *et al*., 2018; Ma *et al*.,
    2020a)， さまざまな防御技術が開発されている（Dai *et al*., 2018; Zhu
    *et al*., 2019a; Tang *et al*., 2019; Jin *et al*., 2020b）．

-   前に述べた通り，スケーラビリティはGNNにとって喫緊の課題である．
    GNNを大規模グラフに対応できるようにするため，多くの研究がなされている（Chen
    *et al*., 2018a, b; Huang *et al*., 2018）．

-   GNNモデルは，複雑を扱えるように設計されており，へテログラフ（Zhang
    *et al*., 2018b; Wang *et al*., 2019i; Chen *et al*.,
    2019b），二部グラフ（He *et al*., 2019），多次元グラフ（Ma *et al*.,
    2019c），符号付きグラフ（Derr *et al*., 2018），ハイパーグラフ（Feng
    *et al*., 2019b; Yadati *et al*., 2019），動的グラフ（Pareja *et
    al*., 2019）などが対象となっている．

-   オートエンコーダ（Wang *et al*., 2016; Cao *et al*.,
    2016）や，変分オートエンコーダ（Kipf and Welling,
    2016b），リカレントニューラルネットワーク（Tai *et al*., 2015; Liang
    *et al*., 2016），敵対的生成ネットワーク（Wang *et al*.,
    2018a）など，様々な深層学習アーキテクチャがグラフに一般化されている．

-   グラフは普遍的なデータ表現であるため，GNNは自然言語処理，コンピュータビジョン，データマイニング，ヘルスケアなど多くの分野を進歩させるために応用されている．


[メインページ](../../index.markdown)

[章目次](./chap1.md)

[メインページ](../../index.markdown)

[章目次](./chap7.md)
## 7.4. サブグラフごとのサンプリング

層ごとのサンプリング方法では, 最終的なノード表現の計算に関わるノード数を大幅に削減し, 近傍爆発の問題を解決することができる. しかし, 層ごとのサンプリングの性質上, 層から層に集約する際に別の問題を引き起こす可能性がある. 具体的には, 式(7.18)において,  $\mathbf{F}\_i^{(l)}$ を生成するための集約処理がこれから集約される, サンプル抽出された各ノードの $\hat{\mathbf{A}}\_{i, j}$ に依存しているということだである. このことから $\mathbf{F}\_i^{(l)}$ を生成するために $N^l$ 中のすべてのノードが使われるわけではなく, ノード $v_i$ とつながりをもつノードのみが使われるということがわかる. もしノード $v_i$ と $N^l$ 中のサンプル抽出されたノードの間のつながりが非常にスパースだった場合, ノード $v_i$ の表現 $\mathbf{F}\_i^{(l)}$ はうまく学習できない可能性がある. 極端な場合, ノード $v_i$ とつながりをもつノードが $N^l$ 中に存在しないときには, 式(7.18)にしたがうと, この層のノード $v_i$ の表現は0になってしまう. したがって学習中の安定性を改善させるためには, ノード $v_i$ にある程度つながっている $N^l$ をサンプル抽出する必要がある. つまり,  $N^l$ と $N^{l-1}$ のサンプル抽出されたノード間の接続を密にして, すべてのノードに情報を集約する元のノードが存在するようにしておく必要がある. 7.3節で述べた層ごとのサンプリング方法では, 層ごとのサンプリング分布を設計する際にこの点を考慮していない. 連続する層のサンプル抽出されたノード間の接続を改善するためには, 連続する層のサンプリング分布を互いに依存するように設計する必要があるが, これは非常に難しい. 全ての層で同じサンプル抽出されたノード集合を用いることで, 分布つくるのが簡単になる. つまり,  $l=L \ldots, 2$ について,  $N^{l}=N^{l-1}$ とする. サンプル抽出されたノード間がよりつながりやすくなるように, 1つだけサンプリング分布を作り出せばよい. さらに, すべての層について同一のノード集合 $\mathcal{V}\_s$ を採用したとすると, 式(7.18)の層ごとの集約は, サンプルしたノード集合 $\mathcal{V}\_s$ 上のグラフ $\mathcal{G}\_s=\left\\{\mathcal{V}\_s, \mathcal{E}\_s\right\\}$ について, すべての近傍ノードの集計をしていることになる. このようにして生成された誘導グラフ $\mathcal{G}\_s$ は元のグラフ $\mathcal{G}$ のサブグラフなっている. というのも, ノードもエッジも元のグラフの部分集合になっているからである( $\mathcal{V}\_s \subset \mathcal{V}, \mathcal{E}\_s \subset \mathcal{E}$ ). 各バッチごとに $\mathcal{V}\_s$ をサンプル抽出する代わりに,  $\mathcal{G}$ から直接サブグラフ $\mathcal{G}\_s$ をサンプル抽出して, サブグラフ上でモデルの学習を行う. このようにノード表現を取得するためにサブグラフをサンプル抽出しモデルの学習を行う方法を「サブグラフごとのサンプリング」と呼ぶ. サブグラフごとのサンプリング方法には, ことなるサブグラフ $\mathcal{G}\_s$ に注目した様々な方法がある(Chiang et al., 2019; Zeng et al., 2019).

(Chiang et al., 2019)では, METIS(Karypis and Kumar, 1998)やGraclus(Dhillon et al., 2007)などのグラフクラスタリングを使って, 各クラスター内のつながりが, クラスター同士のつながりよりもはるかに大きくなるように, グラフ $\mathcal{G}$ からサブグラフ（クラスター） $\{\mathcal{G}\_s\}$ を切り分けている. SGDを適用するため,  $\{\mathcal{G}\_s\}$ から毎回サブグラフをサンプル抽出し, 次の損失関数に基づいて勾配を計算する:

 

$$
 \mathcal{L}_{G_s}=\sum_{v_i \in \mathcal{V}_I \cap \mathcal{V}_s} \ell\left(f_{G N N}\left(\mathbf{A}_s, \mathbf{F}_s ; \boldsymbol{\Theta}\right)_i, y_i\right) $$


 

ここで, $\mathbf{A}\_s$ と $\mathbf{F}\_s$ はそれぞれ隣接行列とサンプル抽出されたサブグラフ $\mathcal{G}\_s$ の特徴量である. 集合 $\mathcal{V}\_I \cap \mathcal{V}\_s$ は $\mathcal{V}\_s$ 中のラベル付きのノードからなる. このサンプル抽出されたサブグラフ $\mathcal{G}\_s$ 上でSGDを1ステップ行うのに必要なメモリは $O\left(\left\|\mathcal{E}\_s\right\|+L \cdot\left\|\mathcal{V}\_s\right\| \cdot d+L \cdot d^{2}\right)$ である.

(Zeng et al., 2019)では, サブグラフ $\mathcal{G}\_s$ が誘導されるノード集合 $\mathcal{V}\_s$ をサンプル抽出するために様々なノード抽出法が提案されている. 具体的には, エッジーに基づいたノード抽出法は, 互いに強く影響し合うノードをペアとしてサンプルするように設計されており, ランダムウォークに基づいた方法では, サンプル抽出されたノード間がより接続するように設計されている. これら2つについて簡単に説明する.

-   **エッジベース抽出**

    以下の分布に従って $m$ 個のエッジをランダムにサンプル抽出する:

     

$$
 p((u, v))=\frac{\left(\frac{1}{d(u)+d(v)}\right)}{\sum_{\left(u^{\prime}, v^{\prime}\right) \in \mathcal{E}}\left(\frac{1}{d\left(u^{\prime}\right)+d\left(v^{\prime}\right)}\right)} $$


  ここで,  $d(v)$ はノード $v$ の次元を表す. サンプル抽出されたノード集合 $\mathcal{V}\_s$ は $m$ 個のサンプル抽出されたエッジの端点のノードから構成され, サブグラフ $\mathcal{G}\_s$ を誘導する.

-   **RWベース抽出**

     $\mathcal{V}$ から $r$ 個の根ノード群を一様にサンプル（復元抽出）する. サンプル抽出された各ノードから出発してランダムウォークを生成する. ランダムウォークでたどり着いたノードから最終的なサンプル抽出されたノード集合 $\mathcal{V}\_s$ を構成し, サブグラフ $\mathcal{G}\_s$ を誘導する.

集約処理においては, バイアスを減らすため正規化が行われる:

 

$$
 \mathbf{F}_i^{(l)}=\sum_{v_j \in \mathcal{V}_s} \frac{\hat{\mathbf{A}}_{i, j}}{\alpha_{i, j}} \mathbf{F}_j^{(l-1)} \Theta^{(l-1)} $$


 

ここで,  $\alpha_{i, j}$ はサンプル抽出されたサブグラフから推定される. 具体的には,  $M$ 個のサブグラフの集合について,  $C_i$ と $C_{i,j}$ をノード $v_i$ とエッジ $(v_i, v_j)$ が $M$ 個のグラフに現れる回数とし,  $\alpha_{i, j}$ を $C_{i,j}/C_i$ で推定する. さらに, サンプル抽出されたサブグラフ $\mathcal{G}\_s$ に基づくミニバッチの損失関数も次のように正規化する:

 

$$
 \mathcal{L}_{G_s}=\sum_{v_i \in \mathcal{V}_l \cap \mathcal{V}_s} \frac{1}{\lambda_i} \ell\left(f_{G N N}\left(\mathbf{A}_s, \mathbf{F}_s ; \boldsymbol{\Theta}\right)_i, y_i\right) $$


 

ここで,  $\lambda_i$ は $C_i/M$ で推定できる. この正規化によっても損失関数のバイアスを減らすことができる.


[メインページ](../../index.markdown)

[章目次](./chap7.md)

[前の節へ](./subsection_03.md) [次の節へ](./subsection_05.md)


